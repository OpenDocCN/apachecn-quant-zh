# 十四、用于交易的文本数据——情感分析

这是专门使用**自然语言处理**（**NLP**）和**机器学习**（**ML**从文本数据中提取算法交易策略信号的三章中的第一章。

文本数据的内容非常丰富，但高度非结构化，因此需要更多的预处理，以使 ML 算法能够提取相关信息。一个关键的挑战是将文本转换成数字格式而不丢失其含义。我们将介绍几种能够捕捉语言细微差别的技术，以便将它们用作 ML 算法的输入。

在本章中，我们将介绍基本的**特征提取**技术，这些技术侧重于单个语义单元，即单词或称为**标记**的单词的短组。我们将展示如何通过创建文档术语矩阵将文档表示为标记计数向量，然后继续将其用作**新闻分类**和**情感分析**的输入。我们还将介绍 naive Bayes 算法，这是一种非常流行的算法。

在接下来的两章中，我们将以这些技术为基础，并使用 ML 算法（如主题建模和单词向量嵌入）来捕获更广泛上下文中包含的信息。

特别是在本章中，我们将介绍以下内容：

*   基本 NLP 工作流是什么样子的
*   如何使用 spaCy 和 TextBlob 构建多语言特征提取管道
*   执行 NLP 任务，如**词性**（**词性**）标记或命名实体识别
*   使用文档术语矩阵将标记转换为数字
*   基于朴素贝叶斯模型的文本分类
*   如何进行情感分析

您可以在 GitHub 存储库的相应目录中找到本章的代码示例以及指向其他资源的链接。笔记本电脑包括图像的彩色版本。

# 具有文本数据的 ML–从语言到功能

考虑到人类使用自然语言交流和存储的信息量，文本数据可能非常有价值。与金融投资相关的各种数据源，从公司报表、合同和专利等正式文件，到新闻、观点、分析师研究或评论，再到各种类型的社交媒体帖子或消息。

在线提供了大量不同的文本数据样本来探索 NLP 算法的使用，其中许多都列在本章 GitHub 上的`README`文件中的资源中。有关全面的介绍，请参见 Jurafsky 和 Martin（2008）。

为了实现文本数据的潜在价值，我们将介绍专门的 NLP 技术和最有效的 Python 库，概述处理语言数据的关键挑战，介绍 NLP 工作流的关键元素，并重点介绍与算法交易相关的 NLP 应用程序。

## 使用文本数据的主要挑战

将非结构化文本转换为机器可读格式需要仔细的预处理，以保留数据有价值的语义方面。人类如何理解语言的内容还没有完全理解，提高机器理解语言的能力仍然是一个非常活跃的研究领域。

NLP 尤其具有挑战性，因为 ML 中文本数据的有效使用需要了解语言的内部工作机制以及它所指世界的知识。主要挑战包括：

*   由于**一词多义**而产生的歧义，即一个单词或短语根据上下文具有不同的含义（“当地高中辍学学生被切成两半”）
*   语言的非标准和不断发展的使用，尤其是在社交媒体上
*   **成语**如“认输”的使用
*   棘手的**实体名称**如“臭虫的生命在哪里？”
*   世界知识：“玛丽和苏是姐妹”与“玛丽和苏是母亲”

## NLP 工作流

从文本数据中使用 ML 进行算法交易的一个关键目标是从文档中提取信号。文档是来自相关文本数据源的个样本，例如公司报告、标题、新闻文章或推特。语料库又是文档的集合。

*图 14.1*展示了将文档转换为数据集的**关键步骤**，该数据集可用于训练能够做出可操作预测的监督 ML 算法：

![](img/B15439_14_01.png)

图 14.1：NLP 工作流程

**基本技术**将文本特征提取为独立的语义单元，称为标记，并使用规则和词典对其进行语言和语义信息注释。单词袋模型使用标记频率将文档建模为标记向量，从而生成经常用于文本分类、检索或摘要的文档术语矩阵。

**高级方法**依赖 ML 来细化令牌等基本特性，并生成更丰富的文档模型。其中包括反映跨文档联合使用令牌的主题模型和旨在捕获令牌使用上下文的词向量模型。

在下一节中使用 spaCy 库演示其实现之前，我们将更详细地回顾工作流每个步骤的关键决策和相关权衡。下表总结了 NLP 管道的关键任务：

<colgroup><col> <col></colgroup> 
| 特色 | 描述 |
| 符号化 | 将文本分割为单词、标点符号等。 |
| 词性标注 | 将单词类型指定给标记，例如动词或名词。 |
| 依赖解析 | 标记语法标记依赖项，如 subject<=>object。 |
| 词干化和柠檬化 | 指定单词的基本形式：“was”=>“be”，“rats”=>“rat”。 |
| 句子边界检测 | 找出并切分各个句子。 |
| 命名实体识别 | 标记“真实世界”对象，如人员、公司或位置。 |
| 相似性 | 评估单词、文本跨度和文档的相似性。 |

### 解析和标记文本数据–选择词汇表

标记是给定文档中字符序列的实例，被视为语义单元。词汇是语料库中包含的一组标记，被认为与进一步处理相关；不在词汇表中的标记将被忽略。

当然，**的目标**是提取最准确反映文档含义的标记。这一步的**关键折衷**是选择更大的词汇表，以更好地反映文本源，同时牺牲更多的特征和更高的模型复杂性（在*第 13 章**中讨论为*维度诅咒*数据驱动的风险因子和无监督学习的资产配置*。

这方面的基本选择涉及标点符号和大写字母的处理、拼写更正的使用，以及是否将非常频繁的所谓“停止词”（如“and”或“the”）排除在无意义的噪音之外。

此外，我们需要决定是否将名为*n***-克**的*n*个体标记分组作为语义单元（个体标记也称为*单格*）。两克（或*二元图*的一个例子是“纽约”，而“纽约市”是三克（或*三元图*）。这一决定可以依靠词典或对个别用法和联合用法的相对频率进行比较。代币的独特组合比单字多，因此添加*n*-代币会增加特征数量，并有增加噪音的风险，除非按频率过滤。

### 语言注释——标记之间的关系

语言注释包括应用**句法和语法规则**识别句子的边界（尽管标点不明确），以及标记在句子中的作用和关系，以进行词性标记和依存分析。它还允许识别常见的词根形式，用于词干和柠檬化，以便将相关单词组合在一起。

以下是与注释相关的一些关键概念：

*   **词干生成**使用简单的规则从标记中删除常见的结尾，如*s*、*ly*、*ing*或*ed*，并将其还原为词干或词根形式。
*   **引理化**使用更复杂的规则来推导单词的规范根（**引理**。它可以检测不规则的共同词根，如“更好”和“最好”，更有效地压缩词汇，但比词干提取慢。这两种方法都以牺牲语义细微差别为代价简化了词汇表。
*   **POS**注释有助于根据标记的功能消除歧义（例如，当动词和名词具有相同的形式时），这会增加词汇，但可能会捕捉到有意义的区别。
*   **依赖解析**识别标记之间的层次关系，常用于翻译。这对于需要更高级语言理解的交互式应用程序（如聊天机器人）非常重要。

### 语义注释——从实体到知识图

**命名实体识别****NER**旨在识别代表利益对象的代币，如个人、国家或公司。它可以进一步发展为一个**知识图**，该捕捉这些实体之间的语义和层次关系。例如，它是用于预测新闻事件对情感的影响的应用程序的关键要素。

### 标记–为预测建模分配结果

许多 NLP 应用程序学习根据从文本中提取的有意义信息预测结果。监督学习要求标签向算法传授真正的输入输出关系。对于文本数据，建立这种关系可能具有挑战性，需要明确的数据建模和收集。

例如，决定如何量化文本文档（如电子邮件、转录采访或推特）中隐含的与新领域相关的情感，或者研究文档或新闻报告的哪些方面应指定特定结果。

## 应用

将 ML 与文本数据一起用于交易依赖于以有助于预测未来价格变动的特征形式提取有意义的信息。应用范围从利用新闻的短期市场影响到资产估值驱动因子的长期基本面分析。例子包括：

*   对产品评论情感的评估，以评估公司的竞争地位或行业趋势
*   检测信贷合同中的异常情况，以预测违约的可能性或影响
*   根据方向、规模和受影响实体对新闻影响的预测

例如，摩根大通（JP Morgan）根据 25 万份分析师报告开发了一个预测模型，该模型的表现优于多个基准指数，并产生了与共识每股收益和建议变化形成的情感因子相关的不相关信号。

# 从文本到标记–NLP 管道

在本节中，我们将演示如何使用开源 Python 库 spaCy 构建 NLP 管道。textacy 库建立在 spaCy 的基础上，提供对 spaCy 属性和附加功能的轻松访问。

有关以下代码示例、安装说明和其他详细信息，请参阅笔记本`nlp_pipeline_with_spaCy`。

## 具有空间性和文本性的 NLP 管道

spaCy 是一个广泛使用的 Python 库，具有用于多种语言的快速文本处理的综合功能集。标记化和注释引擎的使用需要安装语言模型。我们将在本章中使用的功能仅需要小型号；更大的模型还包括我们将在*第 16 章*、*盈利电话和 SEC 文件的单词嵌入*中介绍的单词向量。

通过安装并链接库，我们可以实例化 spaCy 语言模型，然后将其应用于文档。结果是一个`Doc`对象，该对象对文本进行标记，并根据可配置的管道组件进行处理，默认情况下，管道组件由标记器、解析器和命名实体识别器组成：

```py
nlp = spacy.load('en')
nlp.pipe_names
['tagger', 'parser', 'ner'] 
```

让我们用一个简单的句子来说明管道：

```py
sample_text = 'Apple is looking at buying U.K. startup for $1 billion'
doc = nlp(sample_text) 
```

### 分析、标记和注释句子

解析的文档内容是可编辑的，每个元素都有个处理管道生成的属性。下一个示例说明了如何访问以下属性：

*   `.text`：原文字文本
*   `.lemma_`：词根
*   `.pos_`：一个基本的 POS 标签
*   `.tag_`：详细的 POS 标签
*   `.dep_`：标记之间的句法关系或依赖关系
*   `.shape_`：单词的形状，以大写、标点和数字表示
*   `.is alpha`：检查令牌是否为字母数字
*   `.is stop`：检查标记是否在给定语言的常用词列表中

我们迭代每个令牌并将其属性分配给一个`pd.DataFrame`：

```py
pd.DataFrame([[t.text, t.lemma_, t.pos_, t.tag_, t.dep_, t.shape_, 
               t.is_alpha, t.is_stop]
              for t in doc],
             columns=['text', 'lemma', 'pos', 'tag', 'dep', 'shape', 
                      'is_alpha', is_stop']) 
```

这将产生以下结果：

<colgroup><col> <col> <col> <col> <col> <col> <col> <col></colgroup> 
| 文本 | 引理 | 销售时点情报系统 | 标签 | 副署长 | 形状 | 是阿尔法吗 | 你停下来了吗 |
| 苹果 | 苹果 | 支柱 | 国民生产净值 | nsubj | Xxxxx | 符合事实的 | 错误的 |
| 是 | 是 | 动词 | VBZ | 辅助的 | xx | 符合事实的 | 符合事实的 |
| 看 | 看 | 动词 | VBG | 根 | xxxx | 符合事实的 | 错误的 |
| 在 | 在 | ADP | 在里面 | 把…准备好 | xx | 符合事实的 | 符合事实的 |
| 购买 | 购买 | 动词 | VBG | pcomp | xxxx | 符合事实的 | 错误的 |
| 英国 | 英国 | 支柱 | 国民生产净值 | 复合物 | X.X。 | 错误的 | 错误的 |
| 启动 | 启动 | 名词 | NN | 多布吉 | xxxx | 符合事实的 | 错误的 |
| 对于 | 对于 | ADP | 在里面 | 把…准备好 | xxx | 符合事实的 | 符合事实的 |
| $ | $ | 符号 | $ | quantmod | $ | 错误的 | 错误的 |
| 1. | 1. | 全国矿工联盟 | 光盘 | 复合物 | D | 错误的 | 错误的 |
| 十亿 | 十亿 | 全国矿工联盟 | 光盘 | 波比 | xxxx | 符合事实的 | 错误的 |

我们可以使用以下方法在浏览器或笔记本中可视化语法依赖关系：

```py
displacy.render(doc, style='dep', options=options, jupyter=True) 
```

前面的代码允许我们获得如下所示的依赖关系树：

![](img/B15439_14_02.png)

图 14.2：空间相关性树

我们可以使用`spacy.explain()`进一步了解属性的含义，例如：

```py
spacy.explain("VBZ") 
verb, 3rd person singular present 
```

### 批处理文件

我们现在将阅读一组更大的 2225 篇 BBC 新闻文章（有关数据源的详细信息，请参见 GitHub），这些文章属于五个类别，存储在单独的文本文件中。我们做了以下工作：

1.  调用`pathlib`模块的`Path`对象的`.glob()`方法。
2.  迭代生成的路径列表。
3.  阅读新闻文章的所有行，不包括第一行的标题。
4.  将清理后的结果追加到列表中：

    ```py
    files = Path('..', 'data', 'bbc').glob('**/*.txt')
    bbc_articles = []
    for i, file in enumerate(sorted(list(files))):
        with file.open(encoding='latin1') as f:
            lines = f.readlines()
            body = ' '.join([l.strip() for l in lines[1:]]).strip()
            bbc_articles.append(body)
    len(bbc_articles)
    2225 
    ```

### 句子边界检测

我们将通过调用第一篇文章中的 NLP 对象来说明句子检测：

```py
doc = nlp(bbc_articles[0])
type(doc)
spacy.tokens.doc.Doc 
```

spaCy 从句法分析树计算句子边界，因此标点符号和大写字母起着重要但非决定性的作用。因此，即使对于标点不好的文本，边界也将与子句边界重合。

我们可以使用`.sents`属性访问解析的句子：

```py
sentences = [s for s in doc.sents]
sentences[:3]
[Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.  ,
 The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.,
 TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.] 
```

### 命名实体识别

spaCy 使用`.ent_type_`属性启用命名实体识别：

```py
for t in sentences[0]:
    if t.ent_type_:
        print('{} | {} | {}'.format(t.text, t.ent_type_, spacy.explain(t.ent_type_)))
Quarterly | DATE | Absolute or relative dates or periods
US | GPE | Countries, cities, states
TimeWarner | ORG | Companies, agencies, institutions, etc. 
```

Textacy 使访问第一篇文章中出现的命名实体变得容易：

```py
entities = [e.text for e in entities(doc)]
pd.Series(entities).value_counts().head()
TimeWarner        7
AOL               5
fourth quarter    3
year-earlier      2
one               2 
```

### N 克

N-grams 组合*N*个连续令牌。这对于单词袋模型很有用，因为根据文本上下文，将（例如）“数据科学家”视为单个标记可能比两个不同的标记“数据”和“科学家”更有意义。

文本性使得查看给定长度`n`的`ngrams`至少发生`min_freq`次变得容易：

```py
pd.Series([n.text for n in ngrams(doc, n=2, min_freq=2)]).value_counts()
fourth quarter     3
quarter profits    2
Time Warner        2
company said       2
AOL Europe         2 
```

### spaCy 的流式 API

为了通过处理管道传递更多的文档，我们可以使用 spaCy 的流式 API，如下所示：

```py
iter_texts = (bbc_articles[i] for i in range(len(bbc_articles)))
for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=8)):
    assert doc.is_parsed 
```

### 多语言自然语言处理

spaCy 包括用于英语、德语、西班牙语、葡萄牙语、法语、意大利语和荷兰语的经过训练的语言模型，以及用于命名实体识别的多语言模型。跨语言使用非常简单，因为 API 没有改变。

我们将使用 TED 对话字幕的平行语料库来说明西班牙语模型（数据源参考请参见 GitHub repo）。为此，我们实例化了两种语言模型：

```py
model = {}
for language in ['en', 'es']:
    model[language] = spacy.load(language) 
```

我们在每个模型中读取相应的小文本样本：

```py
text = {}
path = Path('../data/TED')
for language in ['en', 'es']:
    file_name = path /  'TED2013_sample.{}'.format(language)
    text[language] = file_name.read_text() 
```

句子边界检测使用相同的逻辑，但发现不同的细分：

```py
parsed, sentences = {}, {}
for language in ['en', 'es']:
    parsed[language] = model[language](text[language])
    sentences[language] = list(parsed[language].sents)
    print('Sentences:', language, len(sentences[language]))
Sentences: en 22
Sentences: es 22 
```

词性标注的工作方式也相同：

```py
pos = {}
for language in ['en', 'es']:
    pos[language] = pd.DataFrame([[t.text, t.pos_, spacy.explain(t.pos_)] 
                                  for t in sentences[language][0]],
                                 columns=['Token', 'POS Tag', 'Meaning'])
pd.concat([pos['en'], pos['es']], axis=1).head() 
```

这将生成下表：

<colgroup><col> <col> <col> <col> <col> <col></colgroup> 
| 代币 | POS 标签 | 意思 | 代币 | POS 标签 | 意思 |
| 那里 | 副词。 | 副词 | 存在 | 动词 | 动词 |
| s | 动词 | 动词 | 尤娜 | 详细资料 | 限定词 |
| A. | 详细资料 | 限定词 | 伸缩势 | 佐剂病 | 形容词 |
| 牢固的 | 佐剂病 | 形容词 | Y | 康杰 | 结合 |
| 和 | CCONJ | 协调连词 | 索普伦登特 | 佐剂病 | 形容词 |

下一节将说明如何使用经过解析和注释的标记来构建可用于文本分类的文档术语矩阵。

## 带 TextBlob 的 NLP

TextBlob 是一个 Python 库，它为常见的 NLP 任务提供了一个简单的 API，并基于**自然语言工具包**（**NLTK**）和模式web 挖掘库构建。TextBlob 有助于词性标注、名词短语提取、情感分析、分类和翻译等。

为了说明 TextBlob 的用法，我们选取了一篇题为“Robinson 为艰巨任务做好准备”的 BBC 体育文章作为样本。与 spaCy 和其他库类似，第一步是通过`TextBlob`对象表示的管道传递文档，以分配各种任务所需的注释（参见笔记本`nlp_with_textblob`：

```py
from textblob import TextBlob
article = docs.sample(1).squeeze()
parsed_body = TextBlob(article.body) 
```

### 堵塞

为了执行词干分析，我们从 NTLK 库实例化`SnowballStemmer`，对每个标记调用其`.stem()`方法，并显示作为结果修改的标记：

```py
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer('english')
[(word, stemmer.stem(word)) for i, word in enumerate(parsed_body.words) 
 if word.lower() != stemmer.stem(parsed_body.words[i])]
[('Manchester', 'manchest'),
 ('United', 'unit'),
 ('reduced', 'reduc'),
 ('points', 'point'),
 ('scrappy', 'scrappi') 
```

### 情感极性与主体性

TextBlob 使用模式库提供的字典为解析的文档提供极性和主观性估计。这些词典将产品评论中常见的形容词映射到情感极性得分，范围从-1 到+1（否定）↔ 积极）和类似的主观评分（客观↔ 主观的）。

`.sentiment`属性提供了相关代币上每个分数的平均值，而`.sentiment_assessments`属性列出了每个代币的基本值（参见笔记本）：

```py
parsed_body.sentiment
Sentiment(polarity=0.088031914893617, subjectivity=0.46456433637284694) 
```

# 计算代币–文档术语矩阵

在本节中，我们首先介绍 bag of words 模型如何将文本数据转换为数字向量空间表示。目标是通过文档在该空间中的距离来近似文档相似性。然后，我们继续说明如何使用 sklearn 库创建文档术语矩阵。

## 文字袋模型

单词袋模型表示基于其包含的术语或标记频率的文档。每个文档成为一个向量，词汇表中的每个标记对应一个条目，该条目反映了标记与文档的相关性。

### 创建文档术语矩阵

给定词汇表，文档术语矩阵的计算非常简单。然而，它也是一种粗糙的简化，因为它是从词序和语法关系中抽象出来的。尽管如此，它通常能在快速的文本分类中取得良好的效果，因此提供了一个非常有用的起点。

*图 14.3*的左面板说明了该文档模型如何将文本数据转换为带有数字条目的矩阵，其中每行对应一个文档，每列对应词汇表中的一个标记。结果矩阵通常是非常高维和稀疏的，也就是说，它包含许多零条目，因为大多数文档只包含整个词汇表的一小部分。

![](img/B15439_14_03.png)

图 14.3：文件术语矩阵和余弦相似性

有几种方法可以衡量令牌的向量条目，以获取其与文档的相关性。我们将说明如何使用 SKL 学习使用二进制标志来表示存在或不存在，计数和加权计数来解释语料库中所有文档中术语频率的差异。

### 文档相似度的度量

文档作为词向量的表示为每个文档指定了由词汇表创建的向量空间中的一个位置。将向量项解释为该空间中的笛卡尔坐标，我们可以使用两个向量之间的角度来度量它们的相似性，因为指向相同方向的向量包含具有相同频率权重的相同项。

上图的右面板以二维方式简化了向量*d*<sub class="Subscript--PACKT-">1</sub>表示的文档与向量*q*表示的查询向量（一组搜索词或另一文档）之间距离的计算。

**余弦相似性**等于两个向量之间角度的余弦。它将角度的大小转换为[0，1]范围内的数字，因为所有向量项都是非负的标记权重。值 1 表示两个文档的令牌权重相同，而值 0 表示两个文档仅包含不同的令牌。

如图所示，角度的余弦等于矢量的点积，即其坐标的和积除以每个矢量的长度的乘积（按欧几里德范数测量）。

## 使用 scikit 学习记录术语矩阵

scikit 学习预处理模块提供两种工具来创建文档术语矩阵。`CountVectorizer`使用二进制或绝对计数来测量每个文档*d*和令牌*t*的**术语频率**（**TF**）*TF*（*d，t*）。

相比之下，`TfidfVectorizer`通过**逆文档频率**（**IDF**来衡量（绝对）术语频率。因此，出现在更多文档中的术语的权重将低于给定文档中频率相同但在所有文档中频率较低的令牌。更具体地说，使用默认设置，文档术语矩阵的*tf idf*（*d*、*t*项计算为*tf idf（d，t）=tf（d，t）*x*idf（t）*，其中：

![](img/B15439_14_001.png)

其中*n*<sub style="font-style: italic;">d</sub>为文件数量，*df**d*、*t*为*t*项的文件频率。每个文档生成的 TF-IDF 向量相对于其绝对或平方总数进行标准化（有关详细信息，请参阅 sklearn 文档）。TF-IDF 度量最初用于信息检索，用于对搜索引擎结果进行排序，随后被证明对文本分类和聚类非常有用。

这两种工具使用相同的接口，通过生成标记计数填充文档术语矩阵，在对文本进行矢量化之前，对文档列表执行标记化和进一步可选的预处理。

影响词汇表大小的关键参数包括：

*   `stop_words`：使用内置或用户提供的（常用）单词列表进行排除
*   `ngram_range`：包括*n*-克，范围为*n*，由一个元组（*n*min、*n*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">max</sub>定义）
*   `lowercase`：相应转换字符（默认为`True`）
*   `min_df`/`max_df`：忽略出现在较少/较多（`int`）或出现在较少/较多文档中的单词（如果`float`[0.0,1.0]）
*   `max_features`：相应地限制词汇表中标记的数量
*   `binary`：将非零计数设置为 1（`True`）

有关以下代码示例和其他详细信息，请参见笔记本`document_term_matrix`。我们再次使用 2225 篇 BBC 新闻文章作为例证。

### 使用计数矢量器

该笔记本包含一个交互式可视化，探索了`min_df`和`max_df`设置对词汇大小的影响。我们将文章读入一个数据帧，将`CountVectorizer`设置为生成二进制标志并使用所有标记，然后调用其`.fit_transform()`方法生成一个文档术语矩阵：

```py
binary_vectorizer = CountVectorizer(max_df=1.0,
                                    min_df=1,
                                    binary=True)
binary_dtm = binary_vectorizer.fit_transform(docs.body)
<2225x29275 sparse matrix of type '<class 'numpy.int64'>'
   with 445870 stored elements in Compressed Sparse Row format> 
```

输出是一个行格式的`scipy.sparse`矩阵，有效地存储 2225（文档）行和 29275（令牌）列中 445870 个非零条目的一小部分（<0.7%）。

#### 可视化词汇分布

*图 14.4*中的可视化显示，要求标记出现在至少 1%和少于 50%的文档中，将词汇限制在近 30000 个标记中的 10%左右。

这使得每个文档的唯一标记数略多于 100 个，如下图左面板所示。右面板显示剩余令牌的文档频率直方图：

![](img/B15439_14_04.png)

图 14.4：每个文档中唯一令牌的分布和令牌的数量

#### 查找最相似的文档

`CountVectorizer`结果允许我们使用`scipy.spatial.distance`模块提供的成对距离的`pdist()`函数查找最相似的文档。它返回一个压缩的距离矩阵，其条目对应于正方形矩阵的上三角形。我们使用`np.triu_indices()`将距离最小化的索引转换为行和列索引，这些索引依次对应于最近的标记向量：

```py
m = binary_dtm.todense()        # pdist does not accept sparse format
pairwise_distances = pdist(m, metric='cosine')
closest = np.argmin(pairwise_distances)  # index that minimizes distance
rows, cols = np.triu_indices(n_docs)      # get row-col indices
rows[closest], cols[closest]
(6, 245) 
```

第 6 条和第 245 条的余弦相似性最接近，因为它们在 303 个组合词汇表中共享 38 个标记（见笔记本）。下表总结了这两篇文章，并展示了基于字数的相似性度量在识别更深语义相似性方面的有限能力：

<colgroup><col> <col> <col></colgroup> 
|  | 第六条 | 第 245 条 |
| 话题 | 商业 | 商业 |
| 标题 | 美国就业增长仍然缓慢 | EBBER“意识到”世界通信欺诈 |
| 身体 | 美国 1 月份创造的就业机会少于预期，但求职者人数的下降将失业率推至三年来的最低水平。根据劳工部的数据，1 月份美国公司仅增加了 146000 个工作岗位。 | 前世通总裁伯尼·埃伯斯（Bernie Ebbers）与其最亲密的合伙人向美国法院表示，他直接参与了该公司 110 亿美元的金融欺诈。前金融总监斯科特·沙利文（Scott Sullivan）在对埃伯斯的刑事审判中作证时，将他的同事牵连到该公司的会计丑闻中。 |

`CountVectorizer`和`TfidfVectorizer`都可以与 spaCy 一起使用，例如，在标记化过程中执行柠檬化和排除某些字符：

```py
nlp = spacy.load('en')
def tokenizer(doc):
    return [w.lemma_ for w in nlp(doc) 
                if not w.is_punct | w.is_space]
vectorizer = CountVectorizer(tokenizer=tokenizer, binary=True)
doc_term_matrix = vectorizer.fit_transform(docs.body) 
```

更多细节和示例，请参见笔记本。

### TFIDF 变压器和 TFIDF 矢量器

`TfidfTransformer`根据`CountVectorizer`生成的令牌计数的文档项矩阵计算TF-IDF 权重。

`TfidfVectorizer`在一个步骤中执行两个计算。它向控制平滑行为的`CountVectorizer`API 添加了一些参数。

对于小文本示例，TFIDF 计算的工作原理如下：

```py
sample_docs = ['call you tomorrow',
               'Call me a taxi',
               'please call me... PLEASE!'] 
```

我们如前所述计算术语频率：

```py
vectorizer = CountVectorizer()
tf_dtm = vectorizer.fit_transform(sample_docs).todense()
tokens = vectorizer.get_feature_names()
term_frequency = pd.DataFrame(data=tf_dtm,
                             columns=tokens)
  call  me  please  taxi  tomorrow  you
0     1   0       0     0         1    1
1     1   1       0     1         0    0
2     1   1       2     0         0    0 
```

文档频率是包含令牌的文档数：

```py
vectorizer = CountVectorizer(binary=True)
df_dtm = vectorizer.fit_transform(sample_docs).todense().sum(axis=0)
document_frequency = pd.DataFrame(data=df_dtm,
                                  columns=tokens)
   call  me  please  taxi  tomorrow  you
0     3   2       1     1         1    1 
```

TF-IDF 权重是这些值的比率：

```py
tfidf = pd.DataFrame(data=tf_dtm/df_dtm, columns=tokens)
   call   me  please  taxi  tomorrow  you
0  0.33 0.00    0.00  0.00      1.00 1.00
1  0.33 0.50    0.00  1.00      0.00 0.00
2  0.33 0.50    2.00  0.00      0.00 0.00 
```

#### 平滑效果

为了避免零分割，`TfidfVectorizer`对文档和术语频率使用平滑处理：

*   `smooth_idf`：在文档频率中添加一个，就像一个额外的文档包含词汇表中的每个标记一样，以防止零分割
*   `sublinear_tf`：应用次线性 tf 缩放，即用 1+log（tf）替换 tf

结合标准重量，结果略有不同：

```py
vect = TfidfVectorizer(smooth_idf=True,
                      norm='l2',  # squared weights sum to 1 by document
                      sublinear_tf=False,  # if True, use 1+log(tf)
                      binary=False)
pd.DataFrame(vect.fit_transform(sample_docs).todense(),
            columns=vect.get_feature_names())
   call   me  please  taxi  tomorrow  you
0  0.39 0.00    0.00  0.00      0.65 0.65
1  0.43 0.55    0.00  0.72      0.00 0.00
2  0.27 0.34    0.90  0.00      0.00 0.00 
```

#### 使用 TfidfVectorizer 总结新闻文章

由于 TF-IDF 向量能够分配有意义的权值，因此 TF-IDF 向量也可用于汇总文本数据。例如，Reddit 的`autotldr`函数基于类似的算法。有关使用 BBC 文章的示例，请参见笔记本。

## 关键经验教训而非经验教训

处理 ML 模型中使用的自然语言的大量技术和选项对应于这种高度非结构化数据源的复杂性。良好的语言特征的工程既有挑战性又有回报，而且可以说是解开隐藏在文本数据中的语义价值的最重要的一步。

在实践中，经验有助于选择去除噪声而不是信号的变换，但可能仍然需要交叉验证和比较不同预处理选择组合的性能。

# 交易用 NLP

一旦使用前面章节讨论的 NLP 技术将文本数据转换为数字特征，文本分类的工作方式与任何其他分类任务一样。

在本节中，我们将把这些预处理技术应用于新闻文章、产品评论和 Twitter 数据，并教各种分类器预测离散的新闻类别、评论分数和情感极性。

首先，我们将介绍 NaiveBayes 模型，这是一种概率分类算法，能够很好地处理单词包模型生成的文本特征。

本节的代码示例在笔记本`news_text_classification`中。

## 朴素贝叶斯分类器

朴素贝叶斯算法在文本分类中非常流行，因为它的低计算成本和内存需求便于在非常大的高维数据集上进行训练。它的预测性能可以与更复杂的模型竞争，提供了一个良好的基线，并以成功的垃圾邮件检测而闻名。

该模型依赖于贝叶斯定理和假设，即给定结果类，各种特征相互独立。换句话说，对于给定的结果，知道一个特征的值（例如，文档中是否存在令牌）不会提供关于另一个特征的值的任何信息。

### 贝叶斯定理复习器

Bayes 定理将一个事件（例如，电子邮件是垃圾邮件而不是良性“火腿”）的条件概率表示为另一个事件（例如，电子邮件包含某些单词），如下所示：

![](img/B15439_14_002.png)

考虑到包含特定词语的电子邮件实际上是垃圾邮件的**后**概率取决于三个因子的相互作用：

*   电子邮件是垃圾邮件的**优先**概率
*   **在垃圾邮件中遇到这些词的可能性**
*   **证据**，即在电子邮件中看到这些单词的概率

要计算后验概率，我们可以忽略证据，因为所有结果（spam 和 ham）都是一样的，而且无条件的前验概率可能很容易计算。

然而，这种可能性对合理规模的词汇量和真实世界的电子邮件语料库构成了无法克服的挑战。原因是不同文档中出现或未出现的单词组合爆炸，这妨碍了计算概率表和为可能性赋值所需的评估。

### 条件独立假设

使模型既易于处理又被称为*naive*的关键假设是，特征独立于结果。为了说明这一点，让我们用“立即汇款”这三个词对电子邮件进行分类，这样 Bayes 定理就变成了：

![](img/B15439_14_003.png)

形式上，假设这三个词在条件上是独立的，意味着如果邮件是垃圾邮件，那么观察到“发送”的概率不受其他词的影响，即，*P*（发送|金钱，现在，垃圾邮件）=*P*（发送|垃圾邮件）。因此，我们可以简化似然函数：

![](img/B15439_14_004.png)

使用“天真”条件独立假设，分子中的每个项都可以直接从训练数据计算为相对频率。分母在不同类别中是恒定的，当需要比较而不是校准后验概率时，可以忽略。随着因子（即特征）数量的增加，先验概率变得不那么相关。

总之，朴素贝叶斯模型的优点是训练和预测速度快，因为参数的数量与特征的数量成正比，并且它们的估计具有封闭形式的解决方案（基于训练数据频率），而不是昂贵的迭代优化。它还具有直观性和一定的可解释性，不需要超参数调整，并且在提供足够信号的情况下对无关特征具有相对鲁棒性。

然而，当独立性假设不成立，文本分类依赖于特征组合，或者特征相互关联时，该模型的性能将很差。

## 新闻文章分类

首先，我们使用我们之前阅读的 BBC 文章对新闻文章分类的朴素贝叶斯模型进行了说明，以获得一个包含 2225 篇来自五个类别的文章的`DataFrame`：

```py
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2225 entries, 0 to 2224
Data columns (total 3 columns):
topic      2225 non-null object
heading    2225 non-null object
body       2225 non-null object 
```

为了训练和评估多项式朴素贝叶斯分类器，我们将数据分割为默认的 75:25 列车测试集比率，确保测试集类别紧密反映列车集：

```py
y = pd.factorize(docs.topic)[0] # create integer class values
X = docs.body
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,
                                                    stratify=y) 
```

我们继续从训练集中学习词汇，并使用带有默认设置的`CountVectorizer`转换两个数据集，以获得近 26000 个特征：

```py
vectorizer = CountVectorizer()
X_train_dtm = vectorizer.fit_transform(X_train)
X_test_dtm = vectorizer.transform(X_test)
X_train_dtm.shape, X_test_dtm.shape
((1668, 25919), (557, 25919)) 
```

培训和预测遵循标准 sklearn 拟合/预测界面：

```py
nb = MultinomialNB()
nb.fit(X_train_dtm, y_train)
y_pred_class = nb.predict(X_test_dtm) 
```

我们使用`accuracy`对多类预测进行评估，发现默认分类器的准确率接近 98%：

```py
accuracy_score(y_test, y_pred_class)
0.97666068222621 
```

## 使用 Twitter 和 Yelp 数据进行情感分析

情感分析是 NLP 和 ML 在交易中最常用的方法之一，因为对资产或其他价格驱动因子的正面或负面观点可能会影响回报。

通常，情感分析的建模方法依赖于字典（TextBlob 库也是如此）或针对特定领域的结果培训的模型。后者通常更可取，因为它允许更具针对性的标签，例如，将文本特征与随后的价格变化联系起来，而不是间接的情感分数。

我们将使用一个带有二元极性标签的 Twitter 数据集和一个带有五点结果量表的大型 Yelp business review 数据集来说明情感分析的 ML。

### 基于 Twitter 数据的二元情感分类

我们使用了一个数据集，该数据集包含来自 2009 年的 160 万条培训和 350 条测试推文，其中算法分配的二元积极和消极情感分数相当平均（更多详细的数据探索请参见笔记本）。

#### 多项式朴素贝叶斯

我们创建了一个包含 934 个标记的文档术语矩阵，如下所示：

```py
vectorizer = CountVectorizer(min_df=.001, max_df=.8, stop_words='english')
train_dtm = vectorizer.fit_transform(train.text)
<1566668x934 sparse matrix of type '<class 'numpy.int64'>'
    with 6332930 stored elements in Compressed Sparse Row format> 
```

然后我们像以前一样训练`MultinomialNB`分类器，并预测测试集：

```py
nb = MultinomialNB()
nb.fit(train_dtm, train.polarity)
predicted_polarity = nb.predict(test_dtm) 
```

结果的准确率超过 77.5%：

```py
accuracy_score(test.polarity, predicted_polarity)
0.7768361581920904 
```

#### 与 TextBlob 情感分数的比较

我们还获得了 tweets 的 TextBlob 情感分数，并注意到（参见*图 14.5*中的左面板），阳性测试tweets 收到的情感估计值显著较高。然后，我们使用`MultinomialNB`模型的`.predict_proba()`方法计算预测概率，并使用曲线下的相应区域**或**AUC**对两个模型进行比较，我们在*第 6 章**机器学习过程*（见*中的右面板）中介绍了这两个模型图 14.5*）。**

![](img/B15439_14_05.png)

图 14.5：习惯情感得分与一般情感得分的准确性

在这种情况下，自定义朴素贝叶斯模型的性能优于 TextBlob，测试 AUC 为 0.848，而 TextBlob 为 0.825。

### 基于 Yelp 业务评论的多类情感分析

最后，我们将情感分析应用于更大的 Yelp business review 数据集，该数据集包含五个结果类（代码和其他细节见笔记本`sentiment_analysis_yelp`。

数据由多个文件组成，其中包含业务、用户、评论和 Yelp 为鼓励数据科学创新而提供的其他方面的信息。

我们将使用 2010-2018 年期间产生的约 600 万条评论（详情见笔记本）。下图显示了每年的评论数量和平均星级数量，以及所有评论中的星级分布。

![](img/B15439_14_06.png)

图 14.6:Yelp 审查的基本探索性分析

到 2017 年，我们将在 10%的数据样本上对各种模型进行培训，并使用 2018 年回顾作为测试集。除了评审文本产生的文本特征外，我们还将使用与给定用户的评审一起提交的其他信息。

#### 结合文本和数字特征

数据集包含各种数字特征（具体实现见笔记本）。

矢量器生成`scipy.sparse`矩阵。为了将矢量化文本数据与其他特征结合起来，我们还需要首先将这些特征转换为稀疏矩阵；许多 sklearn 对象和其他库（如 LightGBM）可以处理这些非常节省内存的数据结构。将稀疏矩阵转换为密集 NumPy 数组可能会导致内存溢出。

大多数变量都是分类的，所以我们使用一种热编码，因为我们有一个相当大的数据集来适应功能的增加。

我们转换编码的数字特征，并将其与文档术语矩阵相结合：

```py
train_numeric = sparse.csr_matrix(train_dummies.astype(np.uint))
train_dtm_numeric = sparse.hstack((train_dtm, train_numeric)) 
```

#### 基准精度

使用最频繁的星数（=5）预测测试集可获得接近 52%的准确率：

```py
test['predicted'] = train.stars.mode().iloc[0]
accuracy_score(test.stars, test.predicted)
0.5196950594793454 
```

#### 多项式朴素贝叶斯模型

接下来，我们使用`CountVectorizer`生成的文档术语矩阵（默认设置）训练朴素贝叶斯分类器。

```py
nb = MultinomialNB()
nb.fit(train_dtm,train.stars)
predicted_stars = nb.predict(test_dtm) 
```

该预测在测试集上产生了 64.7%的准确率，比基准提高了 24.4%：

```py
accuracy_score(test.stars, predicted_stars)
0.6465164206691094 
```

结合文本和其他功能进行训练可将测试精度提高到 0.671。

#### 逻辑回归

在*第 7 章**线性模型——从风险因子到回报预测*中，我们引入了二元逻辑回归。sklearn 还实现了一个具有多项式和一对所有训练选项的多类模型，其中后者为每个类训练一个二进制模型，同时将所有其他类视为负类。多项式选项比“一对所有”实现更快、更准确。

我们使用`lbfgs`解算器评估正则化参数`C`的一系列值，以确定性能最佳的模型，具体如下（详见 sklearn 文档）：

```py
def evaluate_model(model, X_train, X_test, name, store=False):
    start = time()
    model.fit(X_train, train.stars)
    runtime[name] = time() – start
    predictions[name] = model.predict(X_test)
    accuracy[result] = accuracy_score(test.stars, predictions[result])
    if store:
        joblib.dump(model, f'results/{result}.joblib')
Cs = np.logspace(-5, 5, 11)
for C in Cs:
    model = LogisticRegression(C=C, multi_class='multinomial', solver='lbfgs')
    evaluate_model(model, train_dtm, test_dtm, result, store=True) 
```

*图 14.7*显示了验证结果的曲线图。

#### 基于 LightGBM 的多类梯度增强

为了进行比较，我们还训练了一个具有默认设置和`multiclass`目标的 LightGBM 梯度提升树集合：

```py
param = {'objective':'multiclass', 'num_class': 5}
booster = lgb.train(params=param, 
                    train_set=lgb_train, 
                    num_boost_round=500, 
                    early_stopping_rounds=20,
                    valid_sets=[lgb_train, lgb_test]) 
```

#### 预测性能

*图 14.7*显示了组合数据的每个模型的精度。右面板绘制了数据集和不同正则化水平的逻辑回归模型的验证性能。

多项式 logistic 回归表现最好，测试准确率略高于 74%。朴素贝叶斯的表现要差得多。与线性模型相比，默认的 LightGBM 设置没有改善，精度为 0.736。然而，我们可以调整梯度升压模型的超参数，并且可以很好地看到性能改进，使其至少与逻辑回归相一致。不管是哪种方式，结果都提醒我们不要轻视简单、规范化的模型，因为它们不仅可以带来好的结果，而且可以很快实现。

![](img/B15439_14_07.png)

图 14.7：组合数据（所有模型，左）和具有不同正则化的逻辑回归的测试性能

# 总结

在本章中，我们探讨了许多处理非结构化数据的技术和选项，目的是提取语义上有意义的数字特征，以便在 ML 模型中使用。

我们介绍了基本的标记化和注释管道，并用 spaCy 和 TextBlob 说明了它在多种语言中的实现。基于这些结果，我们构建了一个基于单词袋模型的文档模型，将文档表示为数字向量。我们学习了如何细化预处理管道，然后使用矢量化文本数据进行分类和情感分析。

我们还有两章介绍替代文本数据。在下一章中，我们将学习如何使用无监督学习总结文本，以识别潜在主题。然后，在*第 16 章*、*盈利电话和 SEC 文件的单词嵌入*中，我们将学习如何将单词表示为反映单词使用上下文的向量，这项技术已成功地用于为各种分类任务提供更丰富的文本特征。