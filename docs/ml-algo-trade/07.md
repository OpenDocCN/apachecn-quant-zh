# 七、线性模型——从风险因子到回报预测

线性模型家族代表了最有用的假设类别之一。许多广泛应用于算法交易的学习算法依赖于线性预测，因为它们可以有效地训练，对有噪声的金融数据具有相对鲁棒性，并且与金融理论有很强的联系。线性预测也很直观，易于解释，并且通常能很好地拟合数据，或者至少能提供一个良好的基线。

自从勒让德和高斯将线性回归应用于天文学并开始分析它的统计特性以来，线性回归已经有 200 多年的历史了。此后，许多扩展采用了线性回归模型和基线**普通最小二乘法**（**OLS**）方法来学习其参数：

*   **广义线性模型**（**GLM**）通过允许包含误差分布而非正态分布的响应变量，扩展了的应用范围。GLM 包括出现在分类问题中的**分类响应变量**的概率或逻辑模型。
*   更多的**稳健估计方法**支持统计推断，其中数据违反了基线假设，例如，由于时间或观测值之间的相关性。对于包含对同一单位重复观察的面板数据，如一系列资产的历史回报，通常就是这种情况。
*   **收缩方法**旨在提高线性模型的预测性能。他们使用复杂度惩罚，使模型学习的系数产生偏差，目的是减少模型的方差，提高样本外预测性能。

在实践中，线性模型应用于回归和分类问题，目的是进行推理和预测。学术界和行业研究人员已经开发了许多利用线性回归的资产定价模型。应用包括识别推动资产回报的重要因子，以实现更好的风险和表现管理，以及预测不同时期的回报。另一方面，分类问题包括定向价格预测。

在本章中，我们将介绍以下主题：

*   线性回归的工作原理及其假设
*   线性回归模型的训练与诊断
*   用线性回归预测股票收益
*   使用正则化改进预测性能
*   逻辑回归的工作原理
*   将回归转换为分类问题

您可以在 GitHub 存储库的相应目录中找到本章的代码示例以及指向其他资源的链接。笔记本电脑包括图像的彩色版本。

# 从推理到预测

顾名思义，线性回归模型假设输出是输入线性组合的结果。该模型还假设一个随机误差，允许每个观测偏离预期的线性关系。模型不能以确定性的方式完美地描述输入和输出之间的关系的原因包括，例如，缺少变量、测量或数据收集问题。

如果我们想根据样本估计的回归参数得出关于总体中真实（但未观察到）线性关系的统计结论，我们需要添加关于这些误差统计性质的假设。基线回归模型提出了一个强有力的假设，即误差分布在观测值之间是相同的。它还假设错误彼此独立，换句话说，知道一个错误无助于预测下一个错误。假设**独立且同分布**（**IID**）误差意味着其协方差矩阵是单位矩阵乘以表示误差方差的常数。

这些假设保证了 OLS 方法提供的估计不仅是无偏的，而且是有效的，这意味着 OLS 估计在所有线性学习算法中实现了最低的采样误差。然而，这些假设在实践中很少得到满足。

在金融领域，我们经常会遇到在给定横截面上重复观察的面板数据。试图估计一系列资产在一段时间内对一组风险因子的系统暴露，通常会揭示沿时间轴、横截面维度或两者的相关性。因此，出现了替代学习算法，该算法假设误差协方差矩阵比单位矩阵的倍数更复杂。

另一方面，为线性模型学习有偏参数的方法可能产生方差较低的估计，从而提高其预测性能。收缩方法通过应用正则化来降低模型的复杂性，正则化在线性目标函数中增加了一个惩罚项。

该惩罚与系数的绝对大小正相关，因此它们相对于基线情况收缩。系数越大意味着模型越复杂，对输入变化的反应越强烈。当适当校准时，惩罚可以限制模型系数的增长，使其超出从偏差-方差角度来看的最佳值。

首先，我们将介绍线性模型的横截面和面板数据的基线技术，以及在违反关键假设时产生准确估计的重要增强功能。然后，我们将通过估算在算法交易策略开发中普遍存在的因子模型来说明这些方法。最后，我们将关注收缩方法如何应用正则化，并演示如何使用它们预测资产回报和生成交易信号。

# 基线模型——多元线性回归

我们将从模型的规格说明和目标函数、我们可以用来学习其参数的方法以及允许推断和诊断这些假设的统计假设开始。然后，我们将提供扩展，我们可以使用这些扩展使模型适应违反这些假设的情况。其他背景的有用参考资料包括*伍尔德里奇*（*2002*和*2008*。

## 如何制定模型

多元回归模型定义了一个连续结果变量和*p*输入变量之间的线性函数关系，该变量可以是任何类型，但可能需要预处理。相反，多元回归是指多个输入变量的多个输出的回归。

在总体中，对于输出*y*、输入向量![](img/B15439_07_001.png)和误差![](img/B15439_07_002.png)的单个实例，线性回归模型具有以下形式：

![](img/B15439_07_003.png)

系数的解释很简单：系数![](img/B15439_07_093.png)的值是变量*x*i 对输出的部分平均效应，保持所有其他变量不变。

我们还可以用矩阵形式更简洁地编写模型。在这种情况下，*y*是*N*输出观测值的向量，*X*是设计矩阵，在*p*变量上有*N*行观测值加上一列 1s 作为截距，![](img/B15439_07_004.png)是包含*p*的向量+1 系数：

![](img/B15439_07_005.png)

该模型在其*p*+1 参数中是线性的，但如果我们相应地选择或转换变量，例如通过包含多项式基展开或对数项，则可以表示非线性关系。您还可以使用带有伪编码的分类变量，并通过创建新的输入来包含变量之间的交互，输入形式为*x*<sub style="font-style: italic;">i</sub>*x*<sub style="font-style: italic;">j</sub>。

为了从统计角度完成模型的制定，以便我们能够测试关于其参数的假设，我们需要对误差项做出具体假设。我们将在介绍学习参数的最重要方法之后进行此操作。

## 如何训练模型

我们可以使用几种方法从数据中学习模型参数：**普通最小二乘法**（**OLS**）、**最大似然估计法**（**MLE**）和**随机梯度下降法**（**SGD**）。我们将依次介绍每种方法。

### 普通最小二乘法–如何将超平面拟合到数据

最小二乘法是学习超平面参数的原始方法，该超平面参数最接近输入数据的输出。顾名思义，它采用最佳近似来最小化输出值与模型所表示的超平面之间的平方距离之和。

给定数据点的模型预测结果与实际结果之间的差异是**残差**（而真实模型与总体真实输出的偏差称为**误差**。因此，在形式术语中，最小二乘估计方法选择系数向量以最小化**残差平方和**（**RSS**：

![](img/B15439_07_006.png)

因此，最小二乘系数![](img/B15439_07_007.png)计算为：

![](img/B15439_07_008.png)

使 RSS 最小化的最佳参数向量是通过将关于前面表达式的![](img/B15439_07_009.png)的导数设置为零而得到的。假设*X*具有全列秩，这要求输入变量不是线性相关的，因此是可逆的，我们得到一个唯一的解，如下所示：

![](img/B15439_07_010.png)

当*y*和*X*具有零均值时，可以通过减去各自的均值来实现，![](img/B15439_07_009.png)表示输入和输出之间的协方差![](img/B15439_07_012.png)和输出方差![](img/B15439_07_013.png)的比率

还有一种几何上的解释：最小化 RSS 的系数确保残差向量![](img/B15439_07_014.png)与*X*的*P*列所跨越的![](img/B15439_07_015.png)子空间正交，并且估计值![](img/B15439_07_016.png)是到该子空间的正交投影。

### 最大似然估计

最大似然估计（MLE）是一种重要的通用方法，用于估计统计模型的参数。它依赖于似然函数，该函数计算在给定作为模型参数函数的输入数据时，观察输出样本的可能性。可能性与概率的不同之处在于它没有标准化为 0 到 1 的范围。

通过假设误差项的分布，例如标准正态分布，我们可以为多元线性回归示例设置似然函数：

![](img/B15439_07_017.png)

这允许我们计算观察给定输出*y*<sub xmlns:epub="http://www.idpf.org/2007/ops" style="font-style: italic;">i</sub>的条件概率，给定相应的输入向量*x*i 和参数![](img/B15439_07_009.png)![](img/B15439_07_019.png)：

![](img/B15439_07_020.png)

假设输出值是条件独立的，给定输入，样本的可能性与单个输出数据点的条件概率乘积成正比。由于使用和比使用乘积更容易，我们将对数应用于获得**对数似然函数**：

![](img/B15439_07_021.png)

最大似然估计的目标是选择模型参数，使观测到的输出样本的概率最大化，将输入作为给定值。因此，MLE 参数估计是通过最大化对数似然函数得出的：

![](img/B15439_07_022.png)

由于正态分布误差的假设，最大化对数似然函数会产生与最小二乘法相同的参数解。这是因为依赖于参数的唯一表达式是指数中的残差平方。

对于其他分布假设和模型，MLE 将产生不同的结果，正如我们将在关于二元分类的最后一节中看到的，结果遵循伯努利分布。此外，最大似然估计（MLE）是一种更一般的估计方法，因为在许多情况下，最小二乘法不适用，我们将在后面的逻辑回归中看到这一点。

### 梯度下降

梯度下降法是一种通用的优化算法，可以找到光滑函数的平稳点。如果目标函数是凸的，则解将是全局最优解。梯度下降的变化被广泛用于训练复杂的神经网络，也用于计算 MLE 问题的解。

该算法使用目标函数的梯度。梯度包含目标相对于参数的偏导数。这些导数表示目标在相应参数方向上的无穷小（无穷小）步长的变化程度。结果表明，函数值的最大变化源于梯度方向上的一个步骤。

*图 7.1*描绘了一个单变量*x*和一个凸函数*f（x）*的过程，我们正在寻找最小值*x*<sub class="Subscript--PACKT-">0</sub>。当函数具有负斜率时，梯度下降会增加*x*<sub class="Subscript--PACKT-">0</sub>的目标值，否则会减小值：

![](img/B15439_07_01.png)

图 7.1：梯度下降

当我们最小化描述（例如）预测误差代价的函数时，算法使用训练数据计算当前参数值的梯度。然后，它根据其相应梯度分量的负值按比例修改每个参数。因此，目标函数将采用较低的值，并将参数移动到更接近解决方案的位置。当梯度变小，且参数值变化很小时，优化停止。

这些步骤的大小由学习速率决定，这是一个可能需要调整的关键参数。许多实现包括这样一个选项：随着迭代次数的增加，这种学习速率逐渐降低。根据数据的大小，该算法可能在整个数据集上迭代多次。每一次这样的迭代都被称为一个**纪元**。用于停止进一步迭代的历元数和容差是可以调整的额外超参数。

随机梯度下降随机选择一个数据点并计算该数据点的梯度，而不是在较大样本上进行平均以实现加速。还有批处理版本，每个步骤使用一定数量的数据点。

## 高斯-马尔可夫定理

为了评估模型的统计特性并进行推断，我们需要对残差进行假设，这些残差代表了模型无法正确拟合或“解释”的部分输入数据

**高斯-马尔可夫定理**（**GMT**定义了OLS 产生模型参数![](img/B15439_07_023.png)无偏估计所需的假设，以及这些估计在所有横截面数据线性模型中具有最低标准误差的假设。

基线多元回归模型作出以下 GMT 假设（*Wooldridge 2008*：

*   在总体中，线性保持不变，因此![](img/B15439_07_024.png)，其中![](img/B15439_07_025.png)未知但恒定，![](img/B15439_07_026.png)为随机误差。
*   输入变量![](img/B15439_07_027.png)的数据是来自总体的随机样本。
*   输入变量之间没有精确的线性关系。
*   给定任何输入：![](img/B15439_07_029.png)，误差![](img/B15439_07_026.png)的条件平均值为零。
*   同构误差项![](img/B15439_07_030.png)具有恒定方差，给定输入：![](img/B15439_07_031.png)

第四个假设意味着不存在与任何输入变量相关的缺失变量。

在前四个假设（GMT 1-4）下，OLS 方法提供无偏估计。包含一个无关变量不会使截距和斜率估计产生偏差，但忽略一个相关变量会导致参数估计产生偏差。

在 GMT 1-4 下，OLS 也是一致的：随着样本量的增加，随着标准误差变得任意，估计值收敛到真实值。不幸的是，反之亦然：如果由于模型遗漏了相关变量或函数形式错误（例如，遗漏了二次项或对数项），因此误差的条件预期不是零，则所有参数估计都有偏差。如果误差与任何输入变量相关，则 OLS 也不一致，添加更多数据不会消除偏差。

如果我们加入第五个假设，那么 OLS 也会产生**最佳线性无偏估计**（**蓝色**）。最佳是指在所有线性估计量中，估计量的标准误差最低。因此，如果这五个假设成立，并且目标是统计推断，那么 OLS 估计是可行的。然而，如果目标是预测，那么我们将看到存在其他估计器，这些估计器用一些偏差换取较低的方差，以在许多情况下获得更好的预测性能。

现在我们已经介绍了 OLS 的基本假设，我们可以看看小样本和大样本的推断。

## 如何进行统计推断

在线性回归背景下的推理旨在从样本数据中得出关于人群中真实关系的结论。这包括测试关于整体关系的重要性或特定系数的值的假设，以及置信区间的估计。

统计推断的关键成分是具有已知分布的测试统计，通常根据感兴趣的数量（如回归系数）进行计算。在假设假设假设是正确的情况下，我们可以对该统计量建立一个零假设，并计算观察该统计量实际值的概率。这种概率通常被称为即**p 值**：如果它低于显著性阈值（通常为 5%），那么我们拒绝该假设，因为它使得我们在样本中观察到的检验统计值非常不可能。另一方面，p 值反映了我们在拒绝事实上正确的假设时出错的概率。

除了五个 GMT 假设外，**经典线性模型**假设**正态性**——总体误差为正态分布，且与输入变量无关。这一强有力的假设意味着输出变量是正态分布的，取决于输入变量。它允许推导系数的精确分布，这反过来意味着小样本中精确假设测试所需的测试统计数据的精确分布。例如，这种假设在实践中常常失败，因为资产收益率不是正态分布的。

然而，幸运的是，当正态性不成立时，在正态性下使用的测试统计数据也大致有效。更具体地说，测试统计数据的以下分布特征在 GMT 假设 1-5 下大致成立，并且在正态性成立时准确成立：

*   参数估计遵循多元正态分布：![](img/B15439_07_032.png)。
*   在 GMT 1–5 下，参数估计是无偏的，我们可以使用![](img/B15439_07_034.png)得到![](img/B15439_07_033.png)的无偏估计，即恒定误差方差。
*   关于个体系数![](img/B15439_07_035.png)的假设检验的**t 统计量为![](img/B15439_07_036.png)，并遵循*N*-*p*-1 个自由度的*t*分布，其中![](img/B15439_07_037.png)是![](img/B15439_07_038.png)对角线的*j*元素。**
*   *t*分布收敛于正态分布。由于正态分布的 97.5 分位数约为 1.96，参数估计值周围的**95%置信区间的一个有用经验法则是![](img/B15439_07_039.png)，其中*se*表示**标准误差**。包含零的区间意味着我们不能拒绝零假设，即真实参数为零，因此与模型无关。**
*   F-统计量允许测试几个参数的限制，包括整个回归是否显著。它测量由附加变量引起的 RSS 中的变化（减少）。
*   最后，**拉格朗日乘数**（**LM**）测试是测试多个限制的 F 测试的替代测试。

## 如何诊断和解决问题

诊断验证模型假设，并帮助我们在解释结果和进行统计推断时防止错误结论。它们包括拟合优度度量和关于误差项的假设的各种测试，包括残差与正态分布的匹配程度。

此外，诊断评估残差方差是否确实为常数或显示异方差（本节后面将介绍）。他们还测试错误是否有条件地不相关或表现出序列相关性，也就是说，如果知道一个错误有助于预测连续的错误。

除了进行以下诊断测试外，还应始终目视检查残留物。这有助于检测它们是否反映了系统模式，而不是表明模型缺少一个或多个驱动结果的因子的随机噪声。

### 拟合优度

**拟合优度度量**评估模型如何很好地解释结果的变化。例如，在选择不同的模型设计时，它们有助于评估模型规范的质量。

拟合优度指标在衡量拟合的方式上有所不同。在这里，我们将重点关注样本内指标；在下一节中，当我们关注预测模型时，我们将使用样本外测试和交叉验证。

突出的拟合优度度量包括**（调整后）R2**，应最大化，并基于最小二乘估计：

*   R<sup class="Superscript--PACKT-">2</sup>测量模型解释的结果数据中的变化份额，计算为![](img/B15439_07_040.png)，其中 TSS 是结果与其平均值的平方偏差之和。它还对应于实际结果值与模型估计值之间的平方相关系数。内隐目标是最大化 R<sup class="Superscript--PACKT-">2</sup>。然而，当我们添加更多变量时，它永远不会减少。因此，R<sup class="Superscript--PACKT-">2</sup>的缺点之一是，它鼓励过度装配。
*   调整后的 R<sup class="Superscript--PACKT-">2</sup>惩罚 R<sup class="Superscript--PACKT-">2</sup>增加更多变量；每个附加变量都需要显著降低 RSS，以产生更好的拟合优度。

或者，将最小化**Akaike 信息准则**（**AIC**）和**贝叶斯信息准则**（**BIC**）并且基于最大似然估计：

*   ![](img/B15439_07_041.png)，其中![](img/B15439_07_042.png)为最大似然函数的值，*k*为参数个数。
*   ![](img/B15439_07_043.png)，其中*N*为样本量。

这两个指标都因复杂性而受到惩罚。BIC 施加了更高的惩罚，因此它可能相对于 AIC 不合适，反之亦然。

从概念上讲，AIC 的目标是找到最能描述未知数据生成过程的模型，而 BIC 则试图在候选集中找到最佳模型。在实践中，当目标是样本内拟合时，这两个标准可共同用于指导模型选择；否则，交叉验证和基于泛化误差估计的选择更可取。

### 异方差性

GMT 假设 5 要求残差协方差的形状为![](img/B15439_07_044.png)，即对角线矩阵，其条目等于误差项的恒定方差。**异方差**发生在剩余方差不是恒定的，但在不同观测值之间不同的情况下。如果残差方差与输入变量正相关，即当输入值的误差大于其平均值时，则 OLS 标准误差估计值将过低；因此，t 统计量会被夸大，导致错误地发现实际上不存在的关系。

诊断从目视检查残留物开始。（假定为随机）残差中的系统模式表明对无效假设的统计检验，即误差与各种备选方案是同方差的。这些测试包括布鲁希-异教徒和白人测试。

有几种方法可以纠正 OLS 对异方差的估计：

*   **稳健标准误差**（有时称为*白标准误差*）在使用所谓的**夹心估计量**计算误差方差时，将异方差考虑在内。
*   **聚集标准误差**假设数据中有不同的同构组，但不同组之间的误差方差不同。这些集团可能是不同的资产类别或来自不同行业的股票。

当![](img/B15439_07_045.png)时，OLS 的几种替代方法使用不同的假设来估计误差协方差矩阵。以下内容在`statsmodels`中提供：

*   **加权最小二乘法（WLS）**：对于协方差矩阵只有对角线项的异方差误差，如 OLS，但现在允许这些项变化。
*   **可行广义最小二乘法（GLSAR）**：对于遵循自回归 AR（*p*过程的自相关误差（参见*第 9 章*、*波动预测和统计套利时间序列模型*）。
*   **广义最小二乘法（GLS）**：用于任意协方差矩阵结构；在存在异方差或序列相关的情况下，产生有效且无偏的估计。

### 序列相关

序列相关意味着线性回归产生的连续残差是相关的，这违反了第四个 GMT 假设。正序列相关性意味着标准误差被低估，t 统计量将被夸大，如果忽略，将导致错误发现。然而，在计算标准误差时，有一些程序可以校正序列相关性。

**Durbin–Watson 统计**诊断序列相关性。它检验了 OLS 残差不自相关的假设，而不是它们遵循自回归过程（我们将在下一章中探讨）。检验统计量范围为 0～4；接近 2 的值表示非自相关，较低的值表示正自相关，较高的值表示负自相关。准确的阈值取决于参数和观测值的数量，需要在表格中查找。

### 多重共线性

当两个或多个自变量高度相关时，会出现**多重共线性**。这带来了几个挑战：

*   很难确定哪些因子影响因变量。
*   个别 p 值可能会产生误导——p 值可能很高，即使变量事实上很重要。
*   回归系数的置信区间太宽，甚至可能包括零。这使得自变量对结果影响的确定变得复杂。

没有正式的或基于理论的解决方案可以纠正多重共线性。相反，尝试删除一个或多个相关的输入变量，或增加样本大小。

# 如何在实践中运行线性回归

随附的笔记本`linear_regression_intro.ipynb`展示了一个简单的、然后是多元的线性回归，后者使用 OLS 和梯度下降。对于多元回归，我们生成了两个随机输入变量*x*<sub class="Subscript--PACKT-">1</sub>和*x*<sub class="Subscript--PACKT-">2</sub>，范围从-50 到+50，以及一个结果变量，该结果变量作为输入和随机高斯噪声的线性组合计算，以满足正态性假设 GMT 6：

![](img/B15439_07_046.png)

## 具有 stats 模型的 OLS

我们使用`statsmodels`估算一个能够准确反映数据生成过程的多元回归模型，如下所示：

```py
import statsmodels.api as sm
X_ols = sm.add_constant(X)
model = sm.OLS(y, X_ols).fit()
model.summary() 
```

这将产生以下 OLS 回归结果摘要：

![](img/B15439_07_02.png)

图 7.2:OLS 回归结果汇总

摘要的上半部分显示了数据集的特征，即估计方法、观测值和参数的数量，并指出标准误差估计不考虑异方差。中间的面板显示了密切反映人工数据生成过程的系数值。我们可以确认，在汇总结果中间显示的估计可以使用先前导出的 OLS 公式来获得：

```py
beta = np.linalg.inv(X_ols.T.dot(X_ols)).dot(X_ols.T.dot(y))
pd.Series(beta, index=X_ols.columns)
const   53.29
X_1      0.99
X_2      2.96 
```

以下代码可视化了模型如何将模型拟合到随机生成的数据点：

```py
three_dee = plt.figure(figsize=(15, 5)).gca(projection='3d')
three_dee.scatter(data.X_1, data.X_2, data.Y, c='g')
data['y-hat'] = model.predict()
to_plot = data.set_index(['X_1', 'X_2']).unstack().loc[:, 'y-hat']
three_dee.plot_surface(X_1, X_2, to_plot.values, color='black', alpha=0.2, linewidth=1, antialiased=True)
for _, row in data.iterrows():
    plt.plot((row.X_1, row.X_1), (row.X_2, row.X_2), (row.Y, row['y-hat']),              'k-');
three_dee.set_xlabel('$X_1$');three_dee.set_ylabel('$X_2$');three_dee.set_zlabel('$Y, \hat{Y}$') 
```

*图 7.3*显示了生成的超平面和原始数据点：

![](img/B15439_07_03.png)

图 7.3：回归超平面

面板右上部分显示了我们刚才讨论的拟合优度度量，以及 F 检验，它拒绝了所有系数均为零且不相关的假设。同样，t 统计量表明截距和两个斜率系数都非常显著。

摘要的底部包含剩余诊断信息。左侧面板显示歪斜和峰度，用于测试正态性假设。综合检验和 Jarque–Bera 检验都未能拒绝残差正态分布的无效假设。Durbin–Watson 统计检验残差中的序列相关性，其值接近 2，在给定两个参数和 625 个观测值的情况下，无法拒绝无序列相关性的假设，如本主题上一节所述。

最后，条件数提供了多重共线性的证据：它是包含输入数据的设计矩阵的最大和最小特征值的平方根之比。大于 30 的值表明回归可能具有显著的多重共线性。

`statsmodels`包括笔记本中链接的其他诊断测试。

## 基于 sklearn 的随机梯度下降

sklearn 库在其`linear_models`模块中包含一个`SGDRegressor`模型。为了使用这种方法学习同一模型的参数，我们需要标准化数据，因为梯度对比例敏感。

我们使用`StandardScaler()`的目的是：它在拟合步骤中计算每个输入变量的平均值和标准偏差，然后在变换步骤中减去平均值并除以标准偏差，我们可以方便地在单个`fit_transform()`命令中执行：

```py
scaler = StandardScaler()
X_ = scaler.fit_transform(X) 
```

然后，我们使用`random_state`设置之外的默认值实例化`SGDRegressor`，以便于复制：

```py
sgd = SGDRegressor(loss='squared_loss', 
                   fit_intercept=True,
                   shuffle=True, # shuffle data for better estimates
                   random_state=42,
                   learning_rate='invscaling', # reduce rate over time
                   eta0=0.01, # parameters for learning rate path
                   power_t=0.25) 
```

现在，我们可以拟合`sgd`模型，为 OLS 和`sgd`模型创建样本内预测，并计算每个模型的均方根误差：

```py
sgd.fit(X=X_, y=y)
resids = pd.DataFrame({'sgd': y - sgd.predict(X_),
                      'ols': y - model.predict(sm.add_constant(X))})
resids.pow(2).sum().div(len(y)).pow(.5)
ols   48.22
sgd   48.22 
```

正如预期的那样，两个模型都产生了相同的结果。我们现在将着手一个更雄心勃勃的项目，使用线性回归来估计多因子资产定价模型。

# 如何建立线性因子模型

算法交易策略使用因子模型量化资产回报与风险源之间的关系，风险源是这些回报的主要驱动因子。每个因子风险都有溢价，总资产回报率可以预期与这些风险溢价的加权平均值相对应。

因子模型在投资组合管理过程中有几个实际应用，从构建和资产选择到风险管理和表现评估。随着常见风险因子现在可以交易，因子模型的重要性继续增加：

*   对许多资产的收益进行汇总，通过小得多的因子，减少了在优化投资组合时估计协方差矩阵所需的数据量。
*   对资产或投资组合对这些因子的风险敞口的估计允许对产生的风险进行管理，例如，当风险因子本身进行交易或可以代理时，通过输入适当的套期保值。
*   因子模型还允许评估新α因子的增量信号内容。
*   因子模型还可以帮助评估管理者相对于基准的表现是否确实是由熟练的资产选择和市场时机决定的，或者表现是否可以由投资组合向已知回报驱动因子的倾斜来解释。如今，这些驱动因子可以复制为低成本、被动管理、不产生主动管理费用的基金。

以下示例适用于股票，但已确定所有资产类别的风险因子（Ang 2014）。

## 从 CAPM 到 Fama–法国因子模型

自从**资本资产定价模型**（**CAPM**）解释了所有*N*资产![](img/B15439_07_047.png)的预期收益，并使用其各自对单因子的敞口![](img/B15439_07_048.png)来解释，风险因子一直是定量模型的关键组成部分，整体市场超过无风险利率的预期超额回报![](img/B15439_07_049.png)。CAPM 模型采用以下线性形式：

![](img/B15439_07_050.png)

这不同于经典的基本面分析，即多德和格雷厄姆分析，后者的回报取决于公司特征。其基本原理是，总体而言，投资者无法通过多样化消除这种所谓的系统性风险。因此，在均衡状态下，它们需要对持有与其系统风险相称的资产进行补偿。该模型意味着，考虑到价格立即反映所有公共信息的有效市场，不应该有更高的风险调整回报。换句话说，![](img/B15439_07_051.png)的值应该是零。

该模型的实证检验使用线性回归，并一直失败，例如，通过识别不依赖于整体市场敞口的高风险调整回报形式的异常情况，例如较小公司的高回报（Goyal 2012）。

这些失败引发了一场激烈的争论，究竟是有效市场还是联合假说的单因子方面应该受到谴责。事实证明，这两个前提都可能是错误的：

*   约瑟夫·斯蒂格利茨（Joseph Stiglitz）获得 2001 年诺贝尔经济学奖的部分原因是证明了市场通常不是完全有效的：如果市场是有效的，那么收集数据就没有价值，因为这些信息已经反映在价格中。然而，如果没有收集信息的动机，就很难看出它应该如何反映在价格中。
*   另一方面，CAPM 的理论和实证改进表明，额外的因子有助于解释前面提到的一些异常现象，从而产生各种多因子模型。

斯蒂芬·罗斯（Stephen Ross）在 1976 年提出了**套利定价理论**（**APT**），作为一种替代方案，它考虑了多种风险因子，同时避免了市场效率。与 CAPM 相反，它假设可能存在由于错误定价而获得更高回报的机会，但很快就会被套利带走。该理论没有具体说明这些因子，但研究表明，最重要的是通货膨胀和工业生产的变化，以及风险溢价或利率期限结构的变化。

肯尼斯·弗伦奇（Kenneth French）和尤金·法马（Eugene Fama，2013 年诺贝尔奖获得者）发现了依赖于企业特征的其他风险因子，这些因子如今被广泛使用。1993 年，Fama-French 三因子模型将企业的相对规模和价值添加到单一 CAPM 风险源中。2015 年，五因子模型进一步扩展了该集合，将公司盈利能力和投资水平包括在内，这在中间几年被证明是重要的。此外，许多因子模型包括价格动量因子。

根据反映给定风险因子的指标，Fama–French 风险因子计算为具有高值或低值的多样化投资组合的回报差异。根据这些指标对股票进行排序，然后做多某个百分位以上的股票，做空某个百分位以下的股票，即可获得这些回报。与风险因子相关的指标定义如下：

*   **规模**：**市场权益**（**ME**）
*   **价值**：**权益的账面价值**（**为**）除以本人
*   **营业利润率（OP）**：收入减去销售商品/资产成本
*   **投资**：投资/资产

也有无监督学习技术用于数据驱动的风险因子发现，使用因子和主成分分析。我们将在*第 13 章*、*数据驱动风险因子和无监督学习的资产配置*中对此进行探讨。

## 获取风险因子

Fama 和 French 通过其网站提供更新的风险因子和研究组合数据，您可以使用`pandas_datareader`库获取数据。对于此应用程序，请参阅`fama_macbeth.ipynb`笔记本以了解以下代码示例和其他详细信息。

特别是，我们将使用五个 Fama–French 因子，这五个因子是对股票进行排序的结果，首先分为三个大小组，然后再分为两个大小组，分别用于其余三个特定于公司的因子。因此，这些因子涉及三组价值加权投资组合，按规模和账面市值、规模和经营盈利能力以及规模和投资![](img/B15439_07_052.png)分类。下表列出了作为**投资组合**（**PF**的平均回报计算的风险因子值：

<colgroup><col> <col> <col> <col></colgroup> 
| 概念 | 标签 | 名称 | 风险因子计算 |
| 大小 | 中小企业 | 小减大 | 九小股 PF 减去九大股 PF。 |
| 价值 | HML | 高负低 | 二值 PF 减去二增长（具有低 BE/ME 值）PF。 |
| 盈利能力 | RMW | 强负弱 | 两个鲁棒 OP-PF 减去两个弱 OP-PF。 |
| 投资 | CMA | 保守负进取 | 两个保守的投资组合，减去两个积极的投资组合。 |
| 集市 | 风险溢价 | 市场超额回报 | 所有在美国主要交易所注册和上市且数据良好的公司的价值权重回报减去一个月国库券利率。 |

我们将使用 2010-2017 年期间每月获得的回报率，如下所示：

```py
import pandas_datareader.data as web
ff_factor = 'F-F_Research_Data_5_Factors_2x3'
ff_factor_data = web.DataReader(ff_factor, 'famafrench', start='2010', 
                               end='2017-12')[0]
ff_factor_data.info()
PeriodIndex: 96 entries, 2010-01 to 2017-12
Freq: M
Data columns (total 6 columns):
Mkt-RF 96 non-null float64
SMB    96 non-null float64
HML    96 non-null float64
RMW    96 non-null float64
CMA    96 non-null float64
RF     96 non-null float64 
```

Fama 和 French 还提供了许多投资组合，我们可以用它们来说明对因子风险敞口的估计，以及给定时间段内市场上可用的风险溢价的价值。我们将每月使用一个由 17 个行业投资组合组成的小组。我们将从收益中减去无风险利率，因为因子模型适用于超额收益：

```py
ff_portfolio = '17_Industry_Portfolios'
ff_portfolio_data = web.DataReader(ff_portfolio, 'famafrench', start='2010', 
                                  end='2017-12')[0]
ff_portfolio_data = ff_portfolio_data.sub(ff_factor_data.RF, axis=0)
ff_factor_data = ff_factor_data.drop('RF', axis=1)
ff_portfolio_data.info()
PeriodIndex: 96 entries, 2010-01 to 2017-12
Freq: M
Data columns (total 17 columns):
Food     96 non-null float64
Mines    96 non-null float64
Oil      96 non-null float64
...
Rtail    96 non-null float64
Finan    96 non-null float64
Other    96 non-null float64 
```

现在，我们将基于此面板数据，使用解决一些基本线性回归假设失败的方法，建立一个线性因子模型。

## 法玛-麦克白回归

鉴于风险因子和投资组合回报的数据，评估投资组合对这些回报的敞口有助于了解它们在多大程度上推动了投资组合的回报。了解市场为特定因子的风险敞口支付的溢价也很有意义，也就是说，承担这种风险的价值有多大。然后，风险溢价允许我们估计任何投资组合的回报，前提是我们知道或可以假设其因子风险敞口。

更正式地说，我们将有*i*=1、*N*资产或投资组合在*t*=1、*t*期间的回报，并将表示每项资产的超额期间回报。目的是测试*j*=1、*M*因子是否解释了与每个因子相关的超额收益和风险溢价。在我们的案例中，我们有*N*=17 个投资组合和*M*=5 个因子，每个因子有 96 个数据周期。

因子模型是对给定时期内许多股票的估计。由于经典线性回归的基本假设可能不成立，因此在此类横截面回归中可能会出现推理问题。潜在的违规行为包括测量误差、异方差和序列相关性引起的残差协变以及多重共线性（Fama 和 MacBeth 1973）。

为了解决残差相关性引起的推断问题，Fama 和 MacBeth 提出了一种两步方法，用于因子回报率的横截面回归。两阶段 Fama–Macbeth 回归旨在估计市场对特定风险因子敞口的回报溢价。这两个阶段包括：

*   第一阶段：*N*时间序列回归，每种资产或投资组合一次，对其超额收益进行因子回归，以估计因子负荷。矩阵形式下，每项资产：

    ![](img/B15439_07_053.png)

*   第二阶段：*T*横截面回归，每个时间段一次，估计风险溢价。在矩阵形式中，我们获得每个期间的风险溢价向量：

    ![](img/B15439_07_054.png)

现在，我们可以将因子风险溢价计算为时间平均值，并使用风险溢价估计值随时间独立的假设，得到一个 t 统计量来评估其个体显著性：

![](img/B15439_07_055.png)

如果我们有一个非常大的和代表性的交易风险因子数据样本，我们可以使用样本均值作为风险溢价估计。然而，我们通常没有足够长的历史来证明这一点，样本平均值周围的误差幅度可能相当大。Fama–Macbeth 方法利用因子与其他资产的协方差来确定因子溢价。

资产收益的第二个时刻比第一个时刻更容易估计，并且获得更细粒度的数据可以显著提高估计，这与平均估计不同。

我们可以实施第一阶段，以获得 17 个因子荷载估算值，如下所示：

```py
betas = []
for industry in ff_portfolio_data:
    step1 = OLS(endog=ff_portfolio_data.loc[ff_factor_data.index, industry],
                exog=add_constant(ff_factor_data)).fit()
    betas.append(step1.params.drop('const'))
betas = pd.DataFrame(betas,
                     columns=ff_factor_data.columns,
                     index=ff_portfolio_data.columns)
betas.info()
Index: 17 entries, Food  to Other
Data columns (total 5 columns):
Mkt-RF    17 non-null float64
SMB       17 non-null float64
HML       17 non-null float64
RMW       17 non-null float64
CMA       17 non-null float64 
```

对于第二阶段，我们在因子负荷上对投资组合横截面的期间收益进行 96 次回归：

```py
lambdas = []
for period in ff_portfolio_data.index:
    step2 = OLS(endog=ff_portfolio_data.loc[period, betas.index],
                exog=betas).fit()
    lambdas.append(step2.params)
lambdas = pd.DataFrame(lambdas,
                       index=ff_portfolio_data.index,
                       columns=betas.columns.tolist())
lambdas.info()
PeriodIndex: 96 entries, 2010-01 to 2017-12
Freq: M
Data columns (total 5 columns):
Mkt-RF    96 non-null float64
SMB       96 non-null float64
HML       96 non-null float64
RMW       96 non-null float64
CMA       96 non-null float64 
```

最后，我们计算 96 个期间的平均值，以获得我们的因子风险溢价估计值：

```py
lambdas.mean()
Mkt-RF    1.243632
SMB      -0.004863
HML      -0.688167
RMW      -0.237317
CMA      -0.318075
RF       -0.013280 
```

linearmodels 库使用面板数据的各种模型扩展了`statsmodels`，还实现了两阶段 Fama–MacBeth 过程：

```py
model = LinearFactorModel(portfolios=ff_portfolio_data, 
                          factors=ff_factor_data)
res = model.fit() 
```

这为我们提供了相同的结果：

![](img/B15439_07_04.png)

图 7.4:LinearFactorModel 估算汇总

随附的笔记本说明了在估计更大范围的单个股票的风险溢价时，通过使用行业模型来使用分类变量。

# 用收缩率正则化线性回归

当满足高斯-马尔可夫假设时，训练线性回归模型的最小二乘法将产生最佳线性无偏系数估计。类似于 GLS 的变化也表现得很好，即使违反了 OLS 关于误差协方差矩阵的假设。然而，有一些估计器产生有偏差的系数，以减少方差并实现较低的总体概括误差（Hastine、Tibshirani 和 Friedman，2009）。

当线性回归模型包含许多相关变量时，它们的系数将难以确定。这是，因为对 RSS 的大正系数的影响可以通过对相关变量的类似大负系数来抵消。因此，由于高方差导致的预测误差风险增加，因为系数的摆动空间使得模型更可能过度拟合样本。

## 如何避免过度装修

控制过拟合的一种流行技术是**正则化**，它涉及在误差函数中添加惩罚项，以阻止系数达到大值。换句话说，系数的大小限制可以减轻样本外预测的潜在负面影响。我们将遇到所有模型的正则化方法，因为过度拟合是一个普遍存在的问题。

在本节中，我们将介绍收缩方法，这些方法解决了迄今为止讨论的改进线性模型方法的两个动机：

*   **预测精度**：最小二乘估计的低偏差但高方差表明，通过收缩或将某些系数设置为零，可以减少泛化误差，从而用略高的偏差换取模型方差的降低。
*   **解释**：大量预测因子可能会使结果的整体解释或沟通复杂化。最好牺牲一些细节，将模型限制在影响最大的较小参数子集内。

收缩模型通过对其大小施加惩罚来限制回归系数。他们通过在目标函数中添加一个术语![](img/B15439_07_056.png)来实现这一目标。这一术语意味着收缩模型的系数最小化 RSS，加上与系数（绝对）大小正相关的惩罚。

因此，增加的惩罚将线性回归系数转化为约束最小化问题的解决方案，该问题通常采用以下拉格朗日形式：

![](img/B15439_07_057.png)

正则化参数![](img/B15439_07_058.png)确定惩罚效果的大小，即正则化的强度。一旦![](img/B15439_07_059.png)为正，系数将不同于无约束最小二乘参数，意味着有偏差的估计。您应该通过交叉验证自适应地选择超参数![](img/B15439_07_059.png)，以最小化对预期预测误差的估计。我们将在下一节中说明如何执行此操作。

收缩模型的不同之处在于它们计算惩罚的方式，即*S*的函数形式。最常见的版本是**岭回归**，它使用平方系数之和，以及**套索模型**，它基于系数绝对值之和进行惩罚。

**弹性净回归**使用了两者的组合，此处未明确说明。Scikit learn 包括一个与我们将在这里演示的示例非常类似的实现。

## 岭回归的工作原理

岭回归通过向目标函数添加等于平方系数之和的惩罚来收缩回归系数，而平方系数之和又对应于系数向量的 L2 范数（Hoerl 和 Kennard 1970）：

![](img/B15439_07_061.png)

因此，脊线系数定义为：

![](img/B15439_07_062.png)

截距![](img/B15439_07_063.png)已被排除在惩罚之外，以使程序独立于为输出变量选择的原点，否则，向所有输出值添加常数将改变所有斜率参数，而不是平行移位。

通过从每个输入中减去相应的平均值并将结果除以输入的标准偏差，对输入进行标准化非常重要。这是因为脊线解对输入的规模很敏感。岭估计也有一个闭合解，类似于 OLS 情况：

![](img/B15439_07_064.png)

该方案在反演前将标度单位矩阵![](img/B15439_07_065.png)添加到*X*<sup xmlns:epub="http://www.idpf.org/2007/ops" style="font-style: italic;">T</sup>*X*中，即使*X*TT*X*没有满秩，也保证了问题是非奇异的。这是最初引入该估计器时使用该估计器的动机之一。

脊惩罚导致所有参数按比例收缩。在正交输入的情况下，岭估计只是最小二乘估计的缩放版本，即：

![](img/B15439_07_066.png)

使用输入矩阵*X*的**奇异值分解**（**SVD**），我们可以深入了解收缩在非正交的更常见情况下如何影响输入。中心矩阵的 SVD 表示矩阵的主成分（参见*第 13 章*、*数据驱动的风险因子和无监督学习的资产配置*），该矩阵按方差降序捕获数据列空间中的不相关方向。

岭回归缩小了与输入变量对齐相关的系数，使其与数据中方差最大的方向一致。更具体地说，它最大程度地收缩了那些表示与捕获较少方差的主成分对齐的输入的系数。因此，岭回归中隐含的假设是，在预测产出时，数据中变化最大的方向将最具影响力或最可靠。

## 套索回归的原理

套索（Hastine、Tibshirani 和 Wainwright 2015）被称为信号处理中的基追踪，也通过向残差平方和中添加惩罚来收缩系数，但套索惩罚的效果略有不同。套索惩罚是对应于其 L1 范数的系数向量绝对值之和。因此，lasso 估计值的定义如下：

![](img/B15439_07_067.png)

与岭回归类似，输入需要标准化。套索惩罚使解成为非线性的，并且没有像岭回归那样的系数的闭合形式表达式。相反，套索解是一个二次规划问题，并且有高效的算法计算系数的整个路径，这会导致不同的![](img/B15439_07_059.png)值，其计算成本与岭回归相同。

套索惩罚的效果是，随着正则化的增加，一些系数逐渐减少到零。因此，套索可用于连续选择特征子集。

现在让我们继续，将各种线性回归模型付诸实际使用，并生成预测性股票交易信号。

# 如何用线性回归预测收益

在本节中，我们将使用有收缩和无收缩的线性回归来预测收益并生成交易信号。

首先，我们需要创建模型输入和输出。为此，我们将按照*第 4 章*、*金融特征工程——如何研究阿尔法因子*以及不同时间范围的远期回报，创建特征，我们将使用这些特征作为模型的结果。

然后，我们将应用上一节讨论的线性回归模型来说明它们在`statsmodels`和 sklearn 中的用法，并评估它们的预测性能。在下一章中，我们将使用结果来制定交易策略，并演示由机器学习模型驱动的策略回溯测试的端到端过程。

## 准备模型特征和远期收益

为了为我们的预测模型准备数据，我们需要：

*   选择一个股票和时间范围的宇宙
*   构建和转换我们将用作功能的 alpha 因子
*   计算我们旨在预测的远期收益
*   并且（可能）清理我们的数据

笔记本`preparing_the_model_data.ipynb`包含本节的代码示例。

### 创造投资天地

我们将使用 2013 年至 2017 年 Quandl Wiki 美国股票价格数据集中的每日股票数据。有关如何获取数据，请参阅本书 GitHub 存储库根文件夹中`data`目录中的说明。

我们首先加载每日（调整后的）**开盘、高点、低点、收盘和成交量**（**OHLCV**）价格和元数据，其中包括行业信息。使用`DATA_STORE`的路径，您最初保存 Quandl Wiki 数据的地方：

```py
START = '2013-01-01'
END = '2017-12-31'
idx = pd.IndexSlice # to select from pd.MultiIndex
DATA_STORE = '../data/assets.h5'
with pd.HDFStore(DATA_STORE) as store:
    prices = (store['quandl/wiki/prices']
              .loc[idx[START:END, :],
                   ['adj_open', 'adj_close', 'adj_low', 
                    'adj_high', 'adj_volume']]
              .rename(columns=lambda x: x.replace('adj_', ''))
              .swaplevel()
              .sort_index())
    stocks = (store['us_equities/stocks']
              .loc[:, ['marketcap', 'ipoyear', 'sector']]) 
```

我们删除了没有至少 2 年数据的股票：

```py
MONTH = 21
YEAR = 12 * MONTH
min_obs = 2 * YEAR
nobs = prices.groupby(level='ticker').size()
keep = nobs[nobs > min_obs].index
prices = prices.loc[idx[keep, :], :] 
```

接下来，我们清理行业名称，并确保我们只使用包含价格和行业信息的股票：

```py
stocks = stocks[~stocks.index.duplicated() & stocks.sector.notnull()]
# clean up sector names
stocks.sector = stocks.sector.str.lower().str.replace(' ', '_')
stocks.index.name = 'ticker'
shared = (prices.index.get_level_values('ticker').unique()
          .intersection(stocks.index))
stocks = stocks.loc[shared, :]
prices = prices.loc[idx[shared, :], :] 
```

目前，我们只剩下 2265 台股票，每天至少有 2 年的价格数据。首先是`prices`数据帧：

```py
prices.info(null_counts=True)
MultiIndex: 2748774 entries, (A, 2013-01-02) to (ZUMZ, 2017-12-29)
Data columns (total 5 columns):
open      2748774 non-null float64
close     2748774 non-null float64
low       2748774 non-null float64
high      2748774 non-null float64
volume    2748774 non-null float64
memory usage: 115.5+ MB 
```

接下来是`stocks`数据帧：

```py
stocks.info()
Index: 2224 entries, A to ZUMZ
Data columns (total 3 columns):
marketcap    2222 non-null float64
ipoyear      962 non-null float64
sector       2224 non-null object
memory usage: 69.5+ KB 
```

我们将使用（调整后的）美元交易量的 21 天滚动平均值，为我们的模型选择流动性最强的股票。限制股票数量也有利于减少培训和回溯测试时间；排除低成交量的股票也可以降低价格数据的噪音。

计算要求我们将每日收盘价乘以相应的成交量，然后使用`.groupby()`对每个股票进行滚动平均，如下所示：

```py
prices['dollar_vol'] = prices.loc[:, 'close'].mul(prices.loc[:, 'volume'], axis=0)
prices['dollar_vol'] = (prices
                        .groupby('ticker',
                                 group_keys=False,
                                 as_index=False)
                        .dollar_vol
                        .rolling(window=21)
                        .mean()
                        .reset_index(level=0, drop=True)) 
```

然后，我们使用该值对每个日期的股票进行排名，以便我们可以选择，例如，给定日期 100 只交易量最大的股票：

```py
prices['dollar_vol_rank'] = (prices
                             .groupby('date')
                             .dollar_vol
                             .rank(ascending=False)) 
```

### 使用 TA Lib 选择和计算α因子

我们将使用 TA Lib 创建一些动量和波动性因子，如*第 4 章*、*金融特征工程——如何研究阿尔法因子*中所述。

首先，我们增加**相对强度指数**（**相对强度指数**），如下：

```py
prices['rsi'] = prices.groupby(level='ticker').close.apply(RSI) 
```

快速评估表明，对于 100 只交易量最大的股票，RSI 值的平均和中位数 5 天远期收益率确实在下降，分组以反映通常的 30/70 买入/卖出阈值：

```py
(prices[prices.dollar_vol_rank<100]
 .groupby('rsi_signal')['target_5d'].describe()) 
```

<colgroup><col> <col> <col> <col> <col> <col> <col> <col> <col></colgroup> 
| rsi_ 信号 | 计数 | 意思是 | 性病 | 闵 | 25% | 50% | 75% | 最大值 |
| (0, 30] | 4,154 | 0.12% | 1.01% | -5.45% | -0.34% | 0.11% | 0.62% | 4.61% |
| (30, 70] | 107,329 | 0.05% | 0.76% | -16.48% | -0.30% | 0.06% | 0.42% | 7.57% |
| (70, 100] | 10,598 | 0.00% | 0.63% | -8.79% | -0.28% | 0.01% | 0.31% | 5.86% |

然后，我们计算了**布林带**。TA Lib`BBANDS`函数返回三个值，因此我们设置了一个函数，该函数返回具有较高和较低频带的`DataFrame`，用于`groupby()`和`apply()`：

```py
def compute_bb(close):
    high, mid, low = BBANDS(close)
    return pd.DataFrame({'bb_high': high, 'bb_low': low}, index=close.index)
prices = (prices.join(prices
                      .groupby(level='ticker')
                      .close
                      .apply(compute_bb))) 
```

我们取股票价格和上下波林带之间的百分比差，并取对数来压缩分布。目标是反映相对于近期波动趋势的现值：

```py
prices['bb_high'] = prices.bb_high.sub(prices.close).div(prices.bb_high).apply(np.log1p)
prices['bb_low'] = prices.close.sub(prices.bb_low).div(prices.close).apply(np.log1p) 
```

接下来，我们计算**平均真实范围**（**ATR**），它需要三个输入，即高、低和收盘价格。我们对结果进行了标准化，以使指标更具可比性：

```py
def compute_atr(stock_data):
    df = ATR(stock_data.high, stock_data.low, 
             stock_data.close, timeperiod=14)
    return df.sub(df.mean()).div(df.std())
prices['atr'] = (prices.groupby('ticker', group_keys=False)
                 .apply(compute_atr)) 
```

最后，我们生成**移动平均收敛/发散**（**MACD**）指标，它反映了短期指数移动平均与长期指数移动平均之间的差异：

```py
def compute_macd close:
   macd = MACD(close)[0]
    return (macd - np.mean(macd))/np.std(macd)
prices['macd'] = (prices
                  .groupby('ticker', group_keys=False)
                  .close
                  .apply(lambda x: MACD(x)[0])) 
```

### 添加滞后回报

为了捕捉各种历史滞后的价格趋势，我们计算相应的收益，并将结果转换为每日几何平均数。我们将使用滞后 1 天；1 周和 1 周；1 个月、2 个月和 3 个月。我们还将通过剪切 0.01 和 99.99 百分位的值来实现收益的 winsorize：

```py
q = 0.0001
lags = [1, 5, 10, 21, 42, 63]
for lag in lags:
    prices[f'return_{lag}d'] = (prices.groupby(level='ticker').close
                                .pct_change(lag)
                                .pipe(lambda x: x.clip(lower=x.quantile(q),
                                                       upper=x.quantile(1 - q)
                                                       ))
                                .add(1)
                                .pow(1 / lag)
                                .sub(1)
                                ) 
```

然后，我们将每日（双周）和每月的回报转换为当前观察的特征。换句话说，除了这些时期的最新回报外，我们还使用了前五个结果。例如，我们调整前 5 周的每周收益率，使其与当前观察结果一致，并可用于预测当前的远期收益率：

```py
for t in [1, 2, 3, 4, 5]:
    for lag in [1, 5, 10, 21]:
        prices[f'return_{lag}d_lag{t}'] = (prices.groupby(level='ticker')
                                           [f'return_{lag}d'].shift(t * lag)) 
```

### 生成目标远期收益

我们将测试各种前瞻期的预测。目标是确定产生最佳预测准确度的保持期，通过**信息系数**（**IC**测量。

更具体地说，我们将时间范围*t*的回报向后移动*t*天，将其用作远期回报。例如，我们将 5 天收益率从*t*0 移回*t*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">5</sub>5 天，使该值成为*t*0<sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">0</sub>的模型目标。我们可以生成每日（双周）和每月的远期回报，如下所示：

```py
for t in [1, 5, 10, 21]:
    prices[f'target_{t}d'] = prices.groupby(level='ticker')[f'return_{t}d'].shift(-t) 
```

### 分类变量的虚拟编码

我们需要将任何分类变量转换为数字格式，以便线性回归能够处理它。为此，我们将使用虚拟编码，为每个类别级别创建单独的列，并使用条目 1 标记原始类别列中是否存在该级别，否则为 0。熊猫功能`get_dummies()`自动进行虚拟编码。它检测并正确转换类型为对象的列，如下所示。例如，如果需要为包含整数的列使用伪变量，则可以使用关键字 columns 来标识它们：

```py
df = pd.DataFrame({'categories': ['A','B', 'C']})
  categories
0          A
1          B
2          C
pd.get_dummies(df)
   categories_A  categories_B  categories_C
0             1             0             0
1             0             1             0
2             0             0             1 
```

当将所有类别转换为虚拟变量并使用截距（通常是这样）估计模型时，您会无意中创建多重共线性：矩阵现在包含冗余信息，不再具有满秩，而是变得奇异。

通过删除一个新的指示符列可以很容易地避免这种情况。缺失类别级别上的系数现在将由截距捕获（截距始终为 1，包括每个剩余类别虚拟为 0 时）。

使用`drop_first`关键字相应地更正虚拟变量：

```py
pd.get_dummies(df, drop_first=True)
   categories_B  categories_C
0             0             0
1             1             0
2             0             1 
```

为了捕捉季节性影响和不断变化的市场条件，我们为年度和月度创建时间指标变量：

```py
prices['year'] = prices.index.get_level_values('date').year
prices['month'] = prices.index.get_level_values('date').month 
```

然后，我们将价格数据与行业信息相结合，并为时间和行业类别创建虚拟变量：

```py
prices = prices.join(stocks[['sector']])
prices = pd.get_dummies(prices,
                        columns=['year', 'month', 'sector'],
                        prefix=['year', 'month', ''],
                        prefix_sep=['_', '_', ''],
                        drop_first=True) 
```

因此，我们获得了大约 50 个特征，这些特征现在可以用于上一节讨论的各种回归模型。

## 使用 statsmodels 的线性 OLS 回归

在本节中，我们将演示如何使用`statsmodels`对股票收益数据进行统计推断，并解释结果。笔记本`04_statistical_inference_of_stock_returns_with_statsmodels.ipynb`包含本节的代码示例。

### 选择相关的宇宙

根据我们对美元成交量的排名滚动平均值，我们选择样本中任何给定交易日的前 100 只股票：

```py
data = data[data.dollar_vol_rank<100] 
```

然后我们创建我们的结果变量和特征，如下所示：

```py
y = data.filter(like='target')
X = data.drop(y.columns, axis=1) 
```

### 回归估计

如前所述，我们可以使用 OLS 和`statsmodels`来估计线性回归模型。例如，我们为 5 天的持有期选择远期回报，并相应地拟合模型：

```py
target = 'target_5d'
model = OLS(endog=y[target], exog=add_constant(X))
trained_model = model.fit()
trained_model.summary() 
```

### 诊断统计

您可以在笔记本中查看完整的摘要输出。考虑到大量的功能，为了节省一些空间，我们将省略它，并且只显示诊断统计信息：

```py
=====================================================================
Omnibus:               33104.830   Durbin-Watson:               0.436
Prob(Omnibus):             0.000   Jarque-Bera (JB):      1211101.670
Skew:                     -0.780   Prob(JB):                     0.00
Kurtosis:                 19.205   Cond. No.                     79.8
===================================================================== 
```

诊断统计显示 Jarque–Bera 统计的 p 值较低，表明残差不是正态分布：它们表现出负偏斜和高峰度。*图 7.5*的左面板绘制了剩余分布与正态分布的对比，并突出了这一缺陷。在实践中，这意味着模型产生的错误比“正常”更大：

![](img/B15439_07_05.png)

图 7.5：残差分布和自相关图

此外，Durbin–Watson 统计值较低，为 0.43，因此我们在 5%的水平上轻松地拒绝了“无自相关”的无效假设。因此，残差可能正相关。上图的右面板绘制了前 10 个滞后的自相关系数，指出了滞后 4 之前的显著正相关。这一结果是由于我们结果的重叠：我们预测每天的 5 天回报，因此连续几天的结果包含四个相同的回报。

如果我们的目标是了解哪些因子与远期收益显著相关，我们需要使用稳健标准误差（statsmodels 的`.fit()`方法中的一个参数）重新运行回归，或者完全使用不同的方法，例如允许更复杂的误差协方差的面板模型。

## 使用 scikit 学习的线性回归

由于 sklearn 是针对预测定制的，因此我们将使用交叉验证基于其预测性能评估线性回归模型。您可以在笔记本`05_predicting_stock_returns_with_linear_regression.ipynb`中找到此部分的代码示例。

### 选择特征和目标

我们将为我们的实验选择宇宙，就像我们之前在 OLS 案例中所做的那样，将股票报价限制在任何给定日期按美元价值计算的 100 个交易最多的股票。样本仍然包含 2013-2017 年的 5 年数据。

### 交叉验证模型

我们的数据由许多时间序列组成，每种安全性对应一个时间序列。正如在*第 6 章**机器学习过程*中所讨论的，时间序列等顺序数据需要仔细地进行交叉验证，以便我们不会无意中引入前瞻性偏差或泄漏。

我们可以使用*第 6 章**机器学习过程*中介绍的`MultipleTimeSeriesCV`类来实现这一点。我们使用列车和测试周期的期望长度、我们希望运行的测试周期数以及预测范围内的周期数对其进行初始化。`split()`方法返回一个生成器，生成训练和测试指标对，然后我们可以使用它来选择结果和特征。对的数量取决于参数`n_splits`。

测试周期不重叠，位于数据中可用周期的末尾。使用测试周期后，它将成为向前滚动并保持大小不变的训练数据的一部分。

我们将使用 63 个交易日或 3 个月来测试这一点，以训练模型，然后预测接下来 10 天的 1 天回报。因此，从 2015 年开始，我们可以在 3 年内使用大约 75 次 10 天的拆分。我们将首先定义基本参数和数据结构，如下所示：

```py
train_period_length = 63
test_period_length = 10
n_splits = int(3 * YEAR/test_period_length)
lookahead =1 
cv = MultipleTimeSeriesCV(n_splits=n_splits,
                          test_period_length=test_period_length,
                          lookahead=lookahead,
                          train_period_length=train_period_length) 
```

交叉验证循环迭代`TimeSeriesCV`提供的训练和测试指标，选择特征和结果，训练模型，并预测测试特征的回报。我们还捕获了均方根误差和实际值与预测值之间的斯皮尔曼秩相关性：

```py
target = f'target_{lookahead}d'
lr_predictions, lr_scores = [], []
lr = LinearRegression()
for i, (train_idx, test_idx) in enumerate(cv.split(X), 1):
    X_train, y_train, = X.iloc[train_idx], y[target].iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y[target].iloc[test_idx]
    lr.fit(X=X_train, y=y_train)
    y_pred = lr.predict(X_test)
    preds_by_day = (y_test.to_frame('actuals').assign(predicted=y_pred)
                    .groupby(level='date'))
    ic = preds_by_day.apply(lambda x: spearmanr(x.predicted,
                                                x.actuals)[0] * 100)
    rmese = preds_by_day.apply(lambda x: np.sqrt(
                               mean_squared_error(x.predicted, x.actuals)))
    scores = pd.concat([ic.to_frame('ic'), rmse.to_frame('rmse')], axis=1)

    lr_scores.append(scores)
    lr_predictions.append(preds) 
```

交叉验证过程耗时 2 秒。我们将在下一节中评估结果。

### 评估结果-信息系数和 RMSE

我们已经为我们的宇宙捕获了 3 年的每日测试预测。为了评估模型的预测性能，我们可以通过汇集所有预测来计算每个交易日以及整个期间的信息系数。

*图 7.6*的左面板（见笔记本中的代码）显示了每天计算的秩相关系数的分布，并显示了它们的平均值和中位数，分别接近 1.95 和 2.56。

该图的右侧面板显示了所有测试期间预测和实际 1 天回报的散点图。seaborn`jointplot`估计了一个稳健回归，该回归将较低的权重分配给异常值，并显示出一个小的正相关关系。整个 3 年测试期间的实际和预测回报的秩相关为正，但低至 0.017，且具有统计学显著性：

![](img/B15439_07_06.png)

图 7.6：线性回归的每日和汇总 IC

此外，我们可以跟踪每天 IC 预测的执行情况。*图 7.7*显示了每日信息系数和 RMSE 的 21 天滚动平均值，以及验证期内各自的平均值。这一观点强调，整个时期的小型正 IC 隐藏了-10 到+10 之间的实质性变化：

![](img/B15439_07_07.png)

图 7.7：线性回归模型每日 IC 和 RMSE 的 21 天滚动平均值

## 使用 scikit 学习的岭回归

现在，我们将继续讨论正则化岭模型，我们将使用该模型评估参数约束是否会改善线性回归的预测性能。使用岭模型允许我们选择决定模型目标函数中惩罚项权重的超参数，如前面章节*收缩方法：线性回归正则化*所述。

### 使用交叉验证调整正则化参数

对于岭回归，我们需要使用关键字`alpha`调整正则化参数，该关键字对应于我们之前使用的![](img/B15439_07_058.png)。我们将尝试从 10<sup class="Superscript--PACKT-">-4</sup>到 10<sup class="Superscript--PACKT-">4</sup>的 18 个值，其中较大的值意味着更强的正则化：

```py
ridge_alphas = np.logspace(-4, 4, 9)
ridge_alphas = sorted(list(ridge_alphas) + list(ridge_alphas * 5)) 
```

我们将采用与线性回归相同的交叉验证参数，进行 3 个月的培训，以预测 10 天的每日收益。

岭惩罚的尺度敏感性要求我们使用`StandardScaler`标准化输入。请注意，我们总是使用`.fit_transform()`方法从训练集中学习平均值和标准偏差，然后使用`.transform()`方法将这些学习到的参数应用于测试集。为了自动化预处理，我们创建了一个`Pipeline`，如下面的代码示例所示。我们还收集了脊线系数。否则，交叉验证类似于线性回归过程：

```py
for alpha in ridge_alphas:
    model = Ridge(alpha=alpha,
                  fit_intercept=False,
                  random_state=42)
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)])
    for i, (train_idx, test_idx) in enumerate(cv.split(X), 1):
        X_train, y_train = X.iloc[train_idx], y[target].iloc[train_idx]
        X_test, y_test = X.iloc[test_idx], y[target].iloc[test_idx]
        pipe.fit(X=X_train, y=y_train)
        y_pred = pipe.predict(X_test)
        preds = y_test.to_frame('actuals').assign(predicted=y_pred)
        preds_by_day = preds.groupby(level='date')
        scores = pd.concat([preds_by_day.apply(lambda x: 
                                               spearmanr(x.predicted, 
                                                   x.actuals)[0] * 100)
                            .to_frame('ic'),
                            preds_by_day.apply(lambda x: np.sqrt(
                                                    mean_squared_error(
                                                    y_pred=x.predicted, 
                                                    y_true=x.actuals)))
                            .to_frame('rmse')], axis=1)
        ridge_scores.append(scores.assign(alpha=alpha))
        ridge_predictions.append(preds.assign(alpha=alpha))
        coeffs.append(pipe.named_steps['model'].coef_) 
```

### 交叉验证结果和岭系数路径

现在，我们可以绘制每个超参数值的 IC，以可视化它如何随着正则化的增加而发展。结果表明，我们得到了![](img/B15439_07_070.png)的最高平均值和中值 IC 值。

对于这些正则化水平，*图 7.8*的右面板显示，与![](img/B15439_07_071.png)的（几乎）无约束模型相比，系数略微缩小：

![](img/B15439_07_08.png)

图 7.8：岭回归交叉验证结果

图的左面板显示，就最佳正则化值的平均值和中值 IC 值而言，预测精度仅略有提高。

### 前 10 名系数

系数的标准化使我们能够通过比较其绝对大小得出关于其相对重要性的结论。*图 7.9*显示了使用![](img/B15439_07_072.png)进行正则化的 10 个最相关系数，在所有训练模型上取平均值：

![](img/B15439_07_09.png)

图 7.9：每日 IC 分布和最重要系数

对于这个简单的模型和样本期，滞后的月收益率和各种行业指标发挥了最重要的作用。

## 使用 sklearn 的套索回归

lasso 实现看起来与我们刚才运行的 ridge 模型非常相似。主要的区别在于 lasso 需要使用迭代坐标下降法得出一个解，而岭回归可以依赖于一个封闭形式的解。这会导致更长的训练时间。

### 交叉验证套索模型

交叉验证代码仅与`Pipeline`设置不同。`Lasso`对象允许您设置容差和最大迭代次数，以分别确定它是否已收敛或是否应中止。您还可以依赖于`warm_start`，以便下一次训练从最后一个最佳系数值开始。有关更多详细信息，请参阅 sklearn 文档和笔记本。

我们将使用范围为 10<sup class="Superscript--PACKT-">-10</sup>到 10<sup class="Superscript--PACKT-">-3</sup>的八个`alpha`值：

```py
lasso_alphas = np.logspace(-10, -3, 8)
for alpha in lasso_alphas:
    model = Lasso(alpha=alpha,
                  fit_intercept=False,
                  random_state=42,
                  tol=1e-4,
                  max_iter=1000,
                  warm_start=True,
                  selection='random')
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)]) 
```

### 评估结果–IC 和套索路径

如前所述，我们可以绘制交叉验证期间使用的所有测试集的平均信息系数。我们可以再次看到，正则化改进了无约束模型的 IC，在![](img/B15439_07_073.png)级别上提供了最佳的样本外结果。

最佳正则化值不同于岭回归，因为惩罚由绝对值之和组成，而不是相对较小的系数值的平方值。我们还可以在*图 7.10*中看到，对于该正则化水平，系数也同样缩小，如岭回归情况：

![](img/B15439_07_10.png)

图 7.10：套索交叉验证结果

在这种情况下，lasso 回归的平均和中值 IC 系数略高，而表现最好的模型平均使用一组不同的系数：

![](img/B15439_07_11.png)

图 7.11:Lasso 每日 IC 分布和前 10 个系数

## 比较预测信号的质量

总之，岭回归和套索回归通常会产生类似的结果。岭回归通常计算速度更快，但套索回归也通过逐渐将系数减少到零来提供连续的特征子集选择，从而消除特征。

在此特定设置中，套索回归产生最佳平均值和中值 IC 值，如*图 7.12*所示：

![](img/B15439_07_12.png)

图 7.12：三种模型的日平均 IC 和中位 IC

此外，我们可以使用 Alphalens 计算反映模型预测信号质量的各种度量和可视化，如*第 4 章*、*金融特征工程——如何研究 Alpha 因子*中所述。笔记本`06_evaluating_signals_using_alphalens.ipynb`包含将模型预测与价格信息相结合的代码示例，以生成 Alphalens 所需的α因子输入。

下表显示了根据模型预测的不同五分位数，投资组合的α和β值。在这个简单的示例中，性能差异非常小：

<colgroup><col> <col> <col> <col> <col> <col> <col> <col> <col> <col></colgroup> 
| 米制的 | 阿尔法 |  | 贝塔 |
| 模型 | 1D | 5D | 10 天 | 21D |  | 1D | 5D | 10 天 | 21D |
| 线性回归 | 0.03 | 0.02 | 0.007 | 0.004 |  | -0.012 | -0.081 | -0.059 | 0.019 |
| 岭回归 | 0.029 | 0.022 | 0.012 | 0.008 |  | -0.01 | -0.083 | -0.060 | 0.021 |
| 套索回归 | 0.03 | 0.021 | 0.009 | 0.006 |  | -0.011 | -0.081 | -0.057 | 0.02 |

# 线性分类

目前讨论的线性回归模型假设了一个定量响应变量。在本节中，我们将重点介绍为推理和预测建模定性输出变量的方法，这一过程称为**分类**，在实践中比回归更频繁地发生。

预测数据点的定性响应称为对观察结果进行分类，因为它涉及将观察结果分配给一个类别或类。在实践中，分类方法通常预测定性变量的每个类别的概率，然后使用该概率来决定适当的分类。

我们可以通过忽略输出变量假定为离散值的事实来处理该分类问题，然后应用线性回归模型尝试使用多个输入变量预测分类输出。然而，在这种方法性能非常差的情况下，很容易构造示例。此外，当我们知道![](img/B15439_07_074.png)时，模型产生大于 1 或小于 0 的值并没有直观意义。

有许多不同的分类技术或分类器可用于预测定性响应。在本节中，我们将介绍广泛使用的 logistic 回归，它与线性回归密切相关。我们将在接下来的章节中讨论更复杂的广义加性模型方法，包括决策树和随机森林，以及梯度提升机和神经网络。

## logistic 回归模型

逻辑回归模型源于对输出类的概率建模的愿望，给定一个在*x*中呈线性的函数，就像线性回归模型一样，同时确保它们和为一并保持在【0，1】中，正如我们从概率中所期望的那样。

在本节中，我们将介绍 logistic 回归模型的目标和函数形式，并描述培训方法。然后，我们将说明如何使用`statsmodels`对宏观数据使用逻辑回归进行统计推断，以及如何使用 sklearn 实施的正则化逻辑回归预测价格变动。

### 目标函数

为了说明**目标函数**，我们将使用输出变量*y*，如果股票收益率在给定时间范围*d*内为正值，则该变量的值为 1，否则为 0：

![](img/B15439_07_075.png)

我们可以很容易地将*y*扩展到三个类别，其中 0 和 2 反映了超出某个阈值的负和正价格变动，1 则不然。

逻辑回归不是直接对输出变量*y*建模，而是在给定α因子或特征*x*<sub style="font-style: italic;">t</sub>向量的情况下，对*y*属于任一类别的概率进行建模。换句话说，根据模型中包含的变量值，逻辑回归对股票价格上涨的概率进行建模：

![](img/B15439_07_076.png)

### 物流功能

为了防止模型产生超出[0,1]区间的值，我们必须使用一个函数对*p*（*x*进行建模，该函数只在*x*的整个域中提供 0 到 1 之间的输出。**逻辑函数**满足这一要求，并且总是产生一条 S 形曲线，因此，无论*x*的值如何，我们都将获得一个在概率方面有意义的预测：

![](img/B15439_07_077.png)

这里，向量*x*包括用于由![](img/B15439_07_078.png)的第一分量捕获的截距的 1。我们可以转换此表达式，以分离出看起来像线性回归的部分：

![](img/B15439_07_079.png)

数量*p**x*/[1]− *p*（*x*）被称为**赔率**，这是表示概率的另一种方式，可能在赌博中很常见。这可能具有 0 和![](img/B15439_07_080.png)之间的任何数值优势，其中低值也意味着低概率，高值意味着高概率。

logit 也称为**对数赔率**（因为它是赔率的对数。因此，logistic 回归表示在*x*中呈线性的 logit，与前面的线性回归非常相似。

### 最大似然估计

必须使用可用的训练数据估算系数向量![](img/B15439_07_081.png)。虽然我们可以使用（非线性）最小二乘法来拟合 logistic 回归模型，但最好使用更通用的最大似然法，因为它具有更好的统计特性。正如我们刚才所讨论的，使用最大似然法拟合逻辑回归模型背后的基本直觉是寻找![](img/B15439_07_081.png)的估计值，以便预测概率![](img/B15439_07_083.png)尽可能接近实际结果。换言之，我们试图找到![](img/B15439_07_084.png)，在所有股票价格上涨的情况下，这些估计值都会产生一个接近 1 的数字，否则会产生一个接近 0 的数字。更正式地说，我们寻求最大化似然函数：

![](img/B15439_07_085.png)

与乘积相比，求和更容易，因此，让我们从两侧取对数，以获得对数似然函数和逻辑回归系数的相应定义：

![](img/B15439_07_086.png)

为了最大化这个方程，我们将![](img/B15439_07_087.png)相对于![](img/B15439_07_081.png)的导数设置为零。这就产生了所谓的分数方程*p*+1，其参数是非线性的，可以使用迭代数值方法进行求解。

## 如何使用 statsmodels 进行推理

我们将以包含 1959 年至 2009 年季度美国宏观数据的简单内置数据集为基础，说明如何使用与`statsmodels`进行逻辑回归（详见笔记本`logistic_regression_macro_data`。

下表列出了变量及其转换：

<colgroup><col> <col> <col></colgroup> 
| 变量 | 描述 | 转型 |
| `realgdp` | 实际国内生产总值 | 年增长率 |
| `realcons` | 实际个人消费支出 | 年增长率 |
| `realinv` | 实际国内私人投资总额 | 年增长率 |
| `realgovt` | 实际联邦支出和总投资 | 年增长率 |
| `realdpi` | 实际私人可支配收入 | 年增长率 |
| `m1` | M1 名义货币存量 | 年增长率 |
| `tbilrate` | 每月国库券利率 | 数量 |
| `unemp` | 经季节性调整的失业率（%） | 数量 |
| `infl` | 通货膨胀率 | 数量 |
| `realint` | 实际利率 | 数量 |

为了获得一个二元目标变量，我们计算季度实际 GDP 年增长率的 20 个季度滚动平均值。然后，如果当前增长超过移动平均值，则指定 1，否则指定 0。最后，我们改变指标变量，使下一季度的结果与当前季度一致。

我们使用截距和将四分之一值转换为虚拟变量，并训练逻辑回归模型，如下所示：

```py
import statsmodels.api as sm
data = pd.get_dummies(data.drop(drop_cols, axis=1), columns=['quarter'], drop_first=True).dropna()
model = sm.Logit(data.target, sm.add_constant(data.drop('target', axis=1)))
result = model.fit()
result.summary() 
```

这为我们的模型生成了以下摘要，其中显示了 198 个观测值和 13 个变量，包括一个截距：

![](img/B15439_07_13.png)

图 7.13:Logit 回归结果

总结表明，该模型已使用最大似然法进行训练，并在-67.9 处提供对数似然函数的最大值。

LL 空值-136.42 是当仅包含一个截距时，最大对数似然函数的结果。它构成了**伪 R2 统计量**和**对数似然比**（**LLR**检验的基础。

伪 R<sup class="Superscript--PACKT-">2</sup>统计量是最小二乘法下常见的 R<sup class="Superscript--PACKT-">2</sup>统计量的替代品。根据零模型*m*<sub class="Subscript--PACKT-">0</sub>与全模型*m*<sub class="Subscript--PACKT-">1</sub>的最大对数似然函数之比计算，如下：

![](img/B15439_07_089.png)

这些值从 0（当模型没有提高可能性时）到 1 不等，其中模型完美拟合，对数可能性最大化为 0。因此，值越高表示拟合越好。

LLR 测试通常比较一个更受限制的模型，计算如下：

![](img/B15439_07_090.png)

无效假设是限制模型表现更好，但低 p 值表明我们可以拒绝该假设，而更喜欢完整模型而不是无效模型。这类似于线性回归的 F 检验（当我们使用 MLE 估计模型时，我们也可以使用 LLR 检验）。

在线性回归输出中，z 统计量与 t 统计量起着相同的作用，其计算结果与系数估计值与其标准误差之比相等。假设总体系数为零的零假设![](img/B15439_07_091.png)，p 值也表示观察检验统计量的概率。我们可以针对`intercept`、`realcons`、`realinv`、`realgovt`、`realdpi`和`unemp`拒绝这一假设。

## 用 logistic 回归预测价格变动

套索 L1 惩罚和岭 L2 惩罚均可用于逻辑回归。它们具有与我们刚才讨论的相同的收缩效应，套索可以再次用于任何线性回归模型的变量选择。

与线性回归一样，标准化输入变量很重要，因为正则化模型对尺度敏感。正则化超参数还需要使用交叉验证进行调整，如线性回归。

### 如何将回归转化为分类问题

我们将继续进行价格预测示例，但现在我们将对结果变量进行二值化，使其在 1 天回报为正时取值为 1，否则取值为 0（本节给出的代码示例见笔记本`predicting_price_movements_with_logistic_regression.ipynb`：

```py
target = 'target_1d'
y['label'] = (y[target] > 0).astype(int) 
```

结果略有不平衡，积极的举措多于消极的举措：

```py
y.label.value_counts()
1    56443
0    53220 
```

有了这个新的分类结果变量，我们现在可以使用默认的 L2 正则化来训练逻辑回归。

### logistic 回归超参数的交叉验证

对于逻辑回归，正则化与线性回归相反：较高的![](img/B15439_07_059.png)值意味着较少的正则化，反之亦然。

我们将使用我们的自定义`TimeSeriesCV`交叉验证正则化超参数的 11 个选项，如下所示：

```py
n_splits = 4*252
cv = TimeSeriesCV(n_splits=n_splits,
                  test_period_length=1,
                  train_period_length=252)
Cs = np.logspace(-5, 5, 11) 
```

`train-test`循环现在使用 sklearn 的`LogisticRegression`并计算`roc_auc_score`（详见笔记本）：

```py
for C in Cs:
    model = LogisticRegression(C=C, fit_intercept=True)
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)])
    for i, (train_idx, test_idx) in enumerate(cv.split(X), 1):
        X_train, y_train, = X.iloc[train_idx], y.label.iloc[train_idx]
        pipe.fit(X=X_train, y=y_train)
        X_test, y_test = X.iloc[test_idx], y.label.iloc[test_idx]
        y_score = pipe.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_score=y_score, y_true=y_test) 
```

此外，我们还可以根据预测概率和实际收益计算 IC：

```py
 actuals = y[target].iloc[test_idx]
        ic, pval = spearmanr(y_score, actuals) 
```

### 使用 AUC 和 IC 评估结果

我们可以再次绘制超参数值范围的 AUC 结果。在*图 7.14*中，左面板显示*C*的最佳中位 AUC 结果为 0.1，而最佳平均 AUC 对应于*C*=10<sup class="Superscript--PACKT-">-3</sup>。右侧面板显示*C*=10<sup class="Superscript--PACKT-">4</sup>模型的信息系数分布。这也突出表明，与前面所示的回归模型相比，我们获得的平均值和中值稍高一些：

![](img/B15439_07_14.png)

图 7.14：逻辑回归

在下一章中，我们将使用这些基本模型生成的预测来生成交易策略的信号，并演示如何对其性能进行回溯测试。

# 总结

在本章中，我们介绍了我们的第一个机器学习模型，它使用线性模型的重要基线情况进行回归和分类。我们探索了这两项任务的目标函数公式，学习了各种训练方法，并学习了如何使用模型进行推理和预测。

我们应用这些新的机器学习技术来估计线性因子模型，这些模型对于管理风险、评估新的阿尔法因子和属性性能非常有用。我们还应用线性回归和分类来完成第一个预测任务，即预测绝对和定向条件下的股票收益率。

在下一章中，我们将以交易工作流程的机器学习的形式将我们迄今为止所涵盖的内容放在一起。这一过程从寻找和准备有关特定投资领域的数据和有用特征的计算开始，然后设计和评估机器学习模型，从这些特征中提取可操作的信号，并最终模拟执行和评估将这些信号转化为优化投资组合的策略。