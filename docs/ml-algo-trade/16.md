# 十六、盈利报告和 SEC 文件的文字嵌入

在前两章中，我们使用**字袋模型**将文本数据转换为数字格式。结果是稀疏的、固定长度的向量，表示高维单词空间中的文档。这允许评估文档的相似性，并创建特征来训练模型，以便对文档内容进行分类或对其中表达的情感进行评级。然而，这些向量忽略了使用术语的上下文，因此包含相同单词的两个句子以不同的顺序将由相同的向量编码，即使它们的含义完全不同。

本章介绍另一类算法，使用**神经网络**学习单个语义单元（如单词或段落）的向量表示。这些向量是密集的而不是稀疏的，有几百个实值条目，被称为**嵌入**，因为它们在连续向量空间中为每个语义单元分配了一个位置。他们通过训练一个模型来**根据上下文**预测标记，这样类似的用法意味着类似的嵌入向量。此外，嵌入通过词之间的相对位置编码语义方面，如词之间的关系。因此，它们是深度学习模型的强大功能，用于解决需要语义信息的任务，如机器翻译、问答或维护对话。

为了基于文本数据制定**交易策略，我们通常对文档的含义而非单个代币感兴趣。例如，我们可能希望创建一个数据集，该数据集使用表示推特或带有情感信息的新闻文章的功能（请参阅*第 14 章*、*交易文本数据–情感分析*），或发布后给定期限内的资产回报。尽管单词包模型在编码文本数据时会丢失大量信息，但它具有表示整个文档的优势。然而，单词嵌入已经得到进一步发展，它不仅仅代表单个标记。示例包括**doc2vec**扩展，它采用加权单词嵌入。最近，出现了**注意**机制来产生更多上下文敏感的句子表示，从而产生了**转换器**体系结构，如**伯特**模型家族，它极大地提高了许多自然语言任务的性能。**

更具体地说，在完成本章和相关笔记本之后，您将了解以下内容：

*   什么是单词嵌入，它们是如何工作的，以及它们捕获语义信息的原因
*   如何获取和使用预训练的词向量
*   哪些网络体系结构在培训 word2vec 模型方面最有效
*   如何使用 Keras、Gensim 和 TensorFlow 训练 word2vec 模型
*   可视化和评估词向量的质量
*   如何在 SEC 文件中训练 word2vec 模型来预测股价走势
*   doc2vec 如何扩展 word2vec 并可用于情感分析
*   为什么 transformer 的注意机制对自然语言处理有如此大的影响
*   如何在金融数据上微调预训练的 BERT 模型并提取高质量的嵌入

您可以在本章的 GitHub 目录中找到代码示例和指向其他资源的链接。本章使用神经网络和深度学习；如果不熟悉，您可能需要先阅读*第 17 章*、*交易深度学习*，其中介绍了关键概念和库。

# 单词嵌入如何编码语义

单词袋模型将文档表示为稀疏的高维向量，反映它们所包含的标记。单词嵌入将标记表示为密集的低维向量，因此单词的相对位置反映了它们在上下文中的使用方式。它们体现了语言学中的**分布假设**，即声称单词最好由他们所属的公司来定义。

词向量能够捕捉许多语义方面；不仅同义词被指定在附近嵌入，而且单词可以有多个相似度。例如，“driver”一词可能类似于“motorist”或“factor”。此外，嵌入编码了类似于类比的成对词之间的关系（*东京对日本*就像*巴黎对法国*一样，或者*去就是去*就像*看到的就是看到的*），正如我们将在本节后面说明的那样。

嵌入是通过训练神经网络根据上下文预测单词，反之亦然。在本节中，我们将介绍这些模型是如何工作的，并介绍成功的方法，包括 word2vec、doc2vec 和最新的 transformer 系列模型。

## 神经语言模型如何在上下文中学习用法

单词嵌入是通过训练一个浅层神经网络来预测给定上下文的单词而产生的。传统的语言模型将上下文定义为目标之前的单词，而单词嵌入模型使用目标周围对称窗口中包含的单词。相反，单词袋模型使用整个文档作为上下文，并依赖（加权）计数来捕获单词的共现。

早期使用的神经语言模型包括增加计算复杂性的非线性隐藏层。Mikolov，Sutskever 等人（2013）引入的**word2vec**及其扩展简化了体系结构，以支持对大型数据集的培训。例如，维基百科语料库包含超过 20 亿个代币。（有关前馈网络的更多详细信息，请参见*第 17 章*、*交易深度学习*。）

## word2vec–可扩展的单词和短语嵌入

word2vec 模型是一个两层神经网络，它以文本语料库为输入，并输出一组嵌入向量用于该语料库中的单词。有两种不同的架构，如下图所示，可以使用浅层神经网络有效地学习单词向量（Mikolov，Chen，et al.，2013）：

*   **连续单词袋**（**CBOW**模型使用上下文单词向量的平均值作为输入来预测目标单词，这样它们的顺序就不重要了。CBOW 的训练速度更快，对于频繁使用的词汇，它的准确度更高，但对不频繁使用的词汇关注较少。
*   相反，**skip gram**（**SG**模型使用目标词预测从上下文中抽取的单词。它可以很好地处理小数据集，甚至可以找到罕见单词或短语的良好表示。

![](img/B15439_16_01.png)

图 16.1：连续字包与跳过字包处理逻辑

该模型接收一个嵌入向量作为输入，并用另一个嵌入向量计算点积。注意，假设赋范向量，向量相等时点积最大（绝对值），向量正交时点积最小。

在训练期间，**反向传播算法**根据目标函数基于分类错误计算的损失调整嵌入权重。我们将在下一节中看到 word2vec 如何计算损失。

训练通过在文档上滑动**上下文窗口**来进行，通常被分割成句子。语料库上的每个完整迭代称为**纪元**。根据数据，矢量质量收敛可能需要几十个时代。

skip gram 模型隐式分解了一个单词上下文矩阵，该矩阵包含各个单词和上下文对的逐点互信息（Levy 和 Goldberg，2014）。

### 模型目标–简化 softmax

Word2vec模型旨在从潜在的非常大的词汇表中预测单个单词。神经网络通常使用 softmax 函数作为最后一层的输出单元来实现多类目标，因为它将任意数量的实值映射为相等数量的概率。softmax 函数定义如下，*h*表示嵌入，*v*表示输入向量，*c*表示单词*w*的上下文：

![](img/B15439_16_001.png)

然而，softmax 复杂度随类别数量的增加而增加，因为分母需要计算词汇表中所有单词的点积来标准化概率。Word2vec 通过使用修改版本的 softmax 或基于采样的近似值来提高效率：

*   **分层 softmax**将词汇组织为二叉树，单词作为叶节点。每个节点的唯一路径可用于计算单词概率（Morin 和 Bengio，2005）。
*   **噪声对比估计**（**NCE**）对上下文外的“噪声词”进行采样，并通过二元分类问题逼近多类任务。随着样本数量的增加，NCE 导数接近 softmax梯度，但只有 25 个样本可以产生与 softmax 相似的收敛速度快 45 倍（Mnih 和 Kavukcuoglu，2013）。
*   **负采样**（**负**）省略噪声字样本，逼近 NCE，直接最大化目标字概率。因此，NEG 优化了嵌入向量（类似用法的类似向量）的语义质量，而不是测试集的准确性。然而，与分级 softmax 目标相比，它可能会对不常见的单词产生较差的表示（Mikolov 等人，2013）。

### 自动短语检测

预处理通常涉及短语检测，即识别通常一起使用且应接收单个向量表示的标记（例如，纽约市；参考*第 13 章*、*中对 n-grams 的讨论）数据驱动的风险因子和无监督学习的资产配置*。

最初的 word2vec 作者（Mikolov et al.，2013）使用了一种简单的提升评分法，如果两个单词的联合出现超过了相对于每个单词单独外观的给定阈值，并通过贴现因子*进行校正，则将两个单词*w*<sub style="font-style: italic;">i</sub>、*w*<sub style="font-style: italic;">j</sub>识别为二元图δ*：

![](img/B15439_16_002.png)

记分器可以重复应用，以识别连续较长的短语。

另一种选择是标准化逐点互信息得分，它更准确，但计算成本也更高。它使用相对词频*P*（*w*，在+1 和-1 之间变化：

![](img/B15439_16_003.png)

## 使用语义算法评估嵌入

bag of words 模型创建文档向量，该向量反映标记与文档的存在和相关性。正如*第 15 章*中所述，*主题建模-总结财经新闻*中所述，**潜在语义分析**降低了这些向量的维度，并确定了在此过程中可以解释为潜在概念的内容。**潜在 Dirichlet 分配**将文档和术语表示为包含潜在主题权重的向量。

word2vec 生成的单词和短语向量没有明确的含义。然而，在模型创建的潜在空间中，**嵌入编码类似于邻近性**的用法。嵌入还捕获语义关系，以便通过添加和减去单词向量来表示类比。

*图 16.2*显示了从“巴黎”指向“法国”的向量（测量其嵌入向量之间的差异）如何反映“资本”关系。伦敦和英国之间的类似关系对应于相同的向量：术语“UK”的嵌入非常接近通过将“capital of”向量添加到术语“London”的嵌入中获得的位置：

![](img/B15439_16_02.png)

图 16.2：嵌入向量算法

正如单词可以在不同的语境中使用一样，它们也可以以不同的方式与其他单词相关，而这些关系对应于潜在空间中的不同方向。因此，如果培训数据允许，嵌入应反映几种类型的类比。

word2vec 的作者提供了一份超过 25000 种关系的列表，分为 14 个类别，涵盖地理、语法和语法以及家庭关系等方面，以评估嵌入向量的质量。如上图所示，测试验证了目标单词“UK”与将表示类似关系“巴黎：法国”的向量添加到目标补语“伦敦”的结果最接近。

下表显示了样本数量，并说明了一些类比类别。测试检查*d*的嵌入距离*c+（b-a）*确定的位置有多近。具体实施请参见`evaluating_embeddings`笔记本。

<colgroup><col> <col> <col> <col> <col> <col></colgroup> 
| 类别 | #样品 | A. | B | C | D |
| 首都国家 | 506 | 雅典 | 希腊 | 巴格达 | 伊拉克 |
| 城邦 | 4,242 | 芝加哥 | 伊利诺伊州 | 休斯顿 | 德克萨斯州 |
| 过去时 | 1,560 | 跳舞 | 跳舞 | 减少 | 降低 |
| 复数的 | 1,332 | 香蕉 | 香蕉 | 鸟 | 鸟 |
| 比较的 | 1,332 | 令人不快的 | 更糟的 | 大的 | 更大的 |
| 对面的 | 812 | 可接受的 | 不可接受的 | 意识到的 | 不知道 |
| 最高级 | 1,122 | 令人不快的 | 最差的 | 大的 | 最大的 |
| 复数（动词） | 870 | 减少 | 减少 | 描述 | 描写 |
| 通货 | 866 | 阿尔及利亚 | 第纳尔 | 安哥拉 | 宽扎 |
| 家庭 | 506 | 男孩 | 女孩 | 兄弟 | 姐妹 |

与其他无监督学习技术类似，学习嵌入向量的目的是为其他任务生成特征，如文本分类或情感分析。有两个选项可用于获取给定文档语料库的嵌入向量：

*   使用从维基百科或谷歌新闻等通用大型语料库中学习的预训练嵌入
*   使用反映感兴趣领域的文档培训您自己的模型

后续文本建模任务的内容越不通用且越专业，第二种方法越可取。然而，高质量的单词嵌入需要大量数据，需要包含数亿单词的信息文档。

我们将首先介绍如何使用预训练向量，然后演示如何使用金融新闻和 SEC 文件数据构建自己的 word2vec 模型。

# 如何使用预先训练好的词向量

有几个来源用于预训练单词嵌入。流行的选择包括斯坦福的手套和 spaCy 的内置向量机（详情请参阅`using_pretrained_vectors`笔记本）。在本节中，我们将重点介绍手套。

## 手套–用于单词表示的全局向量

手套（*单词表示的全局向量*、Pennington、Socher 和 Manning，2014）是斯坦福 NLP 实验室开发的一种无监督算法，它从聚合的全局单词共现统计数据中学习单词的向量表示（参见 GitHub 上链接的参考资料）。在以下网络规模来源上预训练的矢量可用：

*   **普通爬网**拥有 420 亿或 8400 亿代币和 190 万或 220 万代币的词汇表
*   **维基百科**2014+Gigaword 5，拥有 60 亿代币和 40 万代币的词汇量
*   **推特**使用 20 亿推特、270 亿代币和 120 万代币的词汇

我们可以使用 Gensim 使用`glove2word2vec`转换矢量文本文件，然后将其加载到`KeyedVector`对象中：

```py
from gensim.models import Word2Vec, KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file=glove_file, word2vec_output_file=w2v_file)
model = KeyedVectors.load_word2vec_format(w2v_file, binary=False) 
```

Gensim 使用上一节中描述的**word2vec 类比测试**，使用作者提供的文本文件评估单词向量。为此，库具有`wv.accuracy`函数，我们使用该函数传递到模拟文件的路径，指示单词向量是否为二进制格式，以及是否要忽略大小写。我们还可以将词汇限制在最频繁的范围内，以加快测试速度：

```py
accuracy = model.wv.accuracy(analogies_path,
                             restrict_vocab=300000,
                             case_insensitive=True) 
```

在维基百科语料库上训练的词向量涵盖了所有类比，总体准确率为 75.44%，但在不同类别之间存在一定差异：

<colgroup><col> <col> <col> <col> <col> <col> <col></colgroup> 
| 类别 | #样品 | 精确 |  | 类别 | #样品 | 精确 |
| 首都国家 | 506 | 94.86% |  | 比较的 | 1,332 | 88.21% |
| 首府区 | 8,372 | 96.46% |  | 对面的 | 756 | 28.57% |
| 城邦 | 4,242 | 60.00% |  | 最高级 | 1,056 | 74.62% |
| 通货 | 752 | 17.42% |  | 现在分词 | 1,056 | 69.98% |
| 家庭 | 506 | 88.14% |  | 过去时 | 1,560 | 61.15% |
| 国籍 | 1,640 | 92.50% |  | 复数的 | 1,332 | 78.08% |
| 形容词副词 | 992 | 22.58% |  | 复数动词 | 870 | 58.51% |

*图 16.3*比较了 100000 个最常见代币的三种手套来源的性能。研究表明，普通的爬行向量覆盖了大约 80%的类比，在 78%的准确率下达到了稍高的。推特向量仅覆盖 25%，准确率为 56.4%：

![](img/B15439_16_03.png)

图 16.3:word2vec 类似物的手套精度

*图 16.4*使用 PCA 将在维基百科语料库上训练的 word2vec 模型的 300 维最密切相关的类比嵌入到二维中，该模型包含超过 20 亿个标记。对以下类别的 24400 多个类似物进行测试，准确率超过 73.5%：

![](img/B15439_16_04.png)

图 16.4：所选模拟嵌入件的二维可视化

# 财经新闻的自定义嵌入

许多任务需要嵌入特定领域的词汇表，而在通用语料库上预先训练的模型可能无法捕获这些词汇表。标准 word2vec 模型无法将向量分配给词汇表外的单词，而是使用默认向量来降低其预测值。

例如，当使用**行业特定文档**时，随着新技术或新产品的出现，词汇或其用法可能会随着时间的推移而改变。因此，嵌入也需要发展。此外，像《企业盈利报告》这样的文件使用了一些微妙的语言，这些语言在维基百科文章中预先训练过，不太可能正确反映出来。

在本节中，我们将使用财经新闻培训和评估特定领域的嵌入。我们将首先演示如何预处理此任务的数据，然后演示第一节中概述的 skip gram 体系结构如何工作，最后将结果可视化。我们还将介绍其他更快的培训方法。

## 预处理-句子检测和 n-grams

为了说明 word2vec 网络架构，我们将使用财经新闻数据集，其中包含超过 125000 篇相关的文章，这些文章是我们在*第 15 章*、*主题建模——总结财经新闻*中介绍的主题建模。我们将加载该章`lda_financial_news.ipynb`笔记本中概述的数据。`financial_news_preprocessing.ipynb`笔记本包含本节的代码示例。

我们使用 spaCy 内置的**句子边界检测**将每篇文章拆分成句子，删除信息量较小的项目，如数字和标点符号，如果结果长度在 6 到 99 个标记之间，则保留结果：

```py
def clean_doc(d):
    doc = []
    for sent in d.sents:
        s = [t.text.lower() for t in sent if not
        any([t.is_digit, not t.is_alpha, t.is_punct, t.is_space])]
        if len(s) > 5 or len(sent) < 100:
            doc.append(' '.join(s))
    return doc
nlp = English()
sentencizer = nlp.create_pipe("sentencizer")
nlp.add_pipe(sentencizer)
clean_articles = []
iter_articles = (article for article in articles)
for i, doc in enumerate(nlp.pipe(iter_articles, batch_size=100, n_process=8), 1):
    clean_articles.extend(clean_doc(doc)) 
```

我们最终得到 243 万个句子，平均包含 15 个标记。

接下来，我们创建 n-gram 来捕获复合项。Gensim 让我们根据关节相对频率和组件的个别出现频率来识别 n-gram。`Phrases`模块对令牌进行评分，`Phraser`类相应地转换文本数据。

它将我们的句子列表转换为一个新的数据集，我们可以将其写入文件，如下所示：

```py
sentences = LineSentence((data_path / f'articles_clean.txt').as_posix())
phrases = Phrases(sentences=sentences,
                          min_count=10,  # ignore terms with a lower count
                          threshold=0.5,  # only phrases with higher score
                          delimiter=b'_',  # how to join ngram tokens
                          scoring='npmi')  # alternative: default
grams = Phraser(phrases)
sentences = grams[sentences]
with (data_path / f'articles_ngrams.txt').open('w') as f:
        for sentence in sentences:
            f.write(' '.join(sentence) + '\n') 
```

笔记本演示了我们如何使用 2 克文件作为输入来重复这个过程，从而创建 3 克。我们最终得到了 250002 克和 15000 3 克或 4 克。检查结果显示得分最高的术语是公司或个人的名字，这表明我们可能需要收紧我们的初始清洁标准。有关数据集的更多详细信息，请参阅笔记本。

## TensorFlow 2 中的 skip-gram 体系结构

在本节中，我们将说明如何使用 TensorFlow 2 的 Keras 接口构建 word2vec 模型，我们将在下一章更详细地介绍该接口。`financial_news_word2vec_tensorflow`笔记本包含代码示例和其他实现细节。

我们首先标记文档并为词汇表中的每个项目分配一个唯一的 ID。首先，我们对上一节中创建的句子子集进行抽样，以限制训练时间：

```py
SAMPLE_SIZE=.5
sentences = file_path.read_text().split('\n')
words = ' '.join(np.random.choice(sentences, size=int(SAMLE_SIZE* l en(sentences)), replace=False)).split() 
```

我们要求语料库中至少出现 10 次，保留 31300 个标记的词汇表，并从以下步骤开始：

1.  提取顶部*n*最常用的单词，学习嵌入。
2.  用唯一整数为这些*n*字编制索引。
3.  创建一个`{index: word}`字典。
4.  将*n*单词替换为其索引，并在其他地方添加一个虚拟值`'UNK'`：

    ```py
    # Get (token, count) tuples for tokens meeting MIN_FREQ
    MIN_FREQ = 10
    token_counts = [t for t in Counter(words).most_common() if t[1] >= MIN_FREQ]
    tokens, counts = list(zip(*token_counts))
    # create id-token dicts & reverse dicts
    id_to_token = pd.Series(tokens, index=range(1, len(tokens) + 1)).to_dict()
    id_to_token.update({0: 'UNK'})
    token_to_id = {t:i for i, t in id_to_token.items()}
    data = [token_to_id.get(word, 0) for word in words] 
    ```

我们最终拥有 1740 万个代币和近 60000 个代币的词汇量，包括多达 3 克的代币。词汇表涵盖了大约 72.5%的类比。

### 噪声对比评估-创建验证样本

Keras 包括一个`make_sampling_table`方法，该方法允许我们创建一个训练集，作为上下文和噪声词对，并带有相应的标签，根据语料库频率进行采样。较低的因子增加了选择频率较低的令牌的概率；笔记本中的图表显示，0.1 的值限制了对前 10000 个令牌的采样：

```py
SAMPLING_FACTOR =  1e-4
sampling_table = make_sampling_table(vocab_size,
                                     sampling_factor=SAMPLING_FACTOR) 
```

### 生成目标上下文词对

为了训练我们的模型，我们需要一对令牌，其中一个表示目标，另一个从周围的上下文窗口中选择，如前面*图 16.1*的右面板所示。我们可以使用 Keras'`skipgrams()`函数如下：

```py
pairs, labels = skipgrams(sequence=data,
                          vocabulary_size=vocab_size,
                          window_size=WINDOW_SIZE,
                          sampling_table=sampling_table,
                          negative_samples=1.0,
                          shuffle=True) 
```

结果是 1.204 亿个上下文目标对，平均分为正样本和负样本。根据我们在上一步中创建的`sampling_table`概率生成负样本。前五个目标和上下文单词 ID 及其匹配标签如下所示：

```py
pd.DataFrame({'target': target_word[:5],
              'context': context_word[:5],
              'label': labels[:5]})
   target context label
0   30867    2117     1
1     196     359     1
2   17960   32467     0
3     314    1721     1
4   28387    7811     0 
```

### 创建 word2vec 模型层

word2vec 模型包含以下内容：

*   接收代表目标上下文对的两个标量值的输入层
*   一个共享嵌入层，用于计算目标词和上下文词向量的点积
*   sigmoid 输出层

**输入层**有两个组件，目标上下文对的每个元素一个组件：

```py
input_target = Input((1,), name='target_input')
input_context = Input((1,), name='context_input') 
```

**共享嵌入层**为词汇表的每个元素包含一个向量，分别根据目标和上下文标记的索引选择：

```py
embedding = Embedding(input_dim=vocab_size,
                      output_dim=EMBEDDING_SIZE,
                      input_length=1,
                      name='embedding_layer')
target = embedding(input_target)
target = Reshape((EMBEDDING_SIZE, 1), name='target_embedding')(target)
context = embedding(input_context)
context = Reshape((EMBEDDING_SIZE, 1), name='context_embedding')(context) 
```

**输出层**通过点积测量两个嵌入向量的相似性，并使用*第 7 章**线性模型——从风险因子到回报预测*中讨论逻辑回归时遇到的`sigmoid`函数转换结果：

```py
# similarity measure
dot_product = Dot(axes=1)([target, context])
dot_product = Reshape((1,), name='similarity')(dot_product)
output = Dense(units=1, activation='sigmoid', name='output')(dot_product) 
```

这个 skip-gram 模型包含一个 200 维的嵌入层，它将为每个词汇表项假定不同的值。结果，我们得到了 59617 x 200 个可训练参数，加上两个用于 sigmoid 输出。

在每次迭代中，该模型计算上下文和目标嵌入向量的点积，将结果通过 sigmoid 生成概率，并根据丢失的梯度调整嵌入。

## 使用张力板可视化嵌入

TensorBoard 是一种可视化工具，允许将嵌入向量投影到两个或三个维度，以探索单词和短语的位置。加载我们创建的嵌入元数据文件（请参阅笔记本）后，您还可以搜索特定术语来查看和探索其邻居，使用 UMAP、t-SNE 或 PCA 将其投影到二维或三维（请参阅*第 13 章，数据驱动的风险因子和无监督学习的资产分配*。有关以下屏幕截图的更高分辨率彩色版本，请参阅笔记本：

![](img/B15439_16_05.png)

图 16.5:3D 嵌入和元数据可视化

## 如何使用 Gensim 更快地培训嵌入

TensorFlow 实现的体系结构非常透明，但速度不是特别快。**自然语言处理**（**NLP**库Gensim，我们在上一章中也用于主题建模，提供了更好的性能，更接近于原作者提供的基于 C 的 word2vec 实现。

用法非常简单。我们首先创建一个句子生成器，它只将我们在预处理步骤中生成的文件名作为输入（我们将再次使用 3-gram）：

```py
sentence_path = data_path / FILE_NAME
sentences = LineSentence(str(sentence_path)) 
```

在第二步中，我们使用熟悉的参数配置 word2vec 模型，这些参数涉及嵌入向量和上下文窗口的大小、最小令牌频率和负样本数等：

```py
model = Word2Vec(sentences,
                 sg=1, # set to 1 for skip-gram; CBOW otherwise
                 size=300,
                 window=5,
                 min_count=20,
                 negative=15,
                 workers=8,
                 iter=EPOCHS,
                 alpha=0.05) 
```

在现代 4 核 i7 处理器上，一次训练需要 2 分钟多一点。

我们可以保留模型和词向量，或者仅保留词向量，如下所示：

```py
# persist model
model.save(str(gensim_path / 'word2vec.model'))
# persist word vectors
model.wv.save(str(gensim_path / 'word_vectors.bin')) 
```

我们可以验证模型性能并继续培训，直到我们对结果满意为止，如下所示：

```py
model.train(sentences, epochs=1, total_examples=model.corpus_count) 
```

在这种情况下，针对六个额外的时期进行的训练会产生最好的结果，在词汇表涵盖的所有类比中，准确率为 41.75%。*图 16.6*的左面板显示了正确/错误的预测以及每个类别的准确度细分。

Gensim 还允许我们评估自定义语义代数。我们可以检查流行的`"woman"+"king"-"man" ~ "queen"`示例如下：

```py
most_sim = best_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=10) 
```

图中右侧面板显示，“女王”是第三个标记，紧随“君主”和不太明显的“刘易斯”之后，然后是几笔版税：

![](img/B15439_16_06.png)

图 16.6：分类和特定示例的类比精度

我们还可以评估与给定目标最相似的标记，以更好地了解嵌入特征。我们根据日志语料频率随机选择：

```py
counter = Counter(sentence_path.read_text().split())
most_common = pd.DataFrame(counter.most_common(), columns=['token', 'count'])
most_common['p'] = np.log(most_common['count'])/np.log(most_common['count']).sum()similars = pd.DataFrame()
for token in np.random.choice(most_common.token, size=10, p=most_common.p):
    similars[token] = [s[0] for s in best_model.wv.most_similar(token)] 
```

下表举例说明了包括几个 n-gram 的结果：

<colgroup><col> <col> <col> <col> <col> <col></colgroup> 
| 目标 | 最接近的匹配 |
| 0 | 1. | 2. | 3. | 4. |
| 轮廓 | 轮廓 | 使用者 | 政治顾问剑桥分析 | 复杂的 | 脸谱网 |
| 撤资 | 剥夺 | 收购 | 接管 | 拜耳 | 巩固 |
| 准备就绪 | 训练 | 军事的 | 命令 | 空军 | 准备工作 |
| 武器 | 核武器 | 俄罗斯联邦 | 弹道导弹 | 武器 | 真主党 |
| 供应中断 | 中断 | 原材料 | 破坏 | 价格 | 经济低迷 |

现在，我们将继续开发一个应用程序，该应用程序使用 SEC 文件与现实交易更密切相关。

# word2vec 用于与 SEC 文件进行交易

在本节中，我们将使用 Gensim 从 SEC 年度文件中学习单词和短语向量，以说明单词嵌入在算法交易中的潜在价值。在以下章节中，我们将把这些向量作为特征与价格回报相结合，训练神经网络，根据证券申报的内容预测股票价格。

特别是，我们将使用一个数据集，其中包含 6500 多家上市公司提交的 2013-2016 年**期间**22000 多份 10-K 年度报告**，包含金融信息和管理评论（见*第 2 章*、*市场和基础数据–来源和技术*。**

对于大约 3000 家公司，对应 11000 份申请，我们有股票价格来标记预测建模的数据。（参见`sec-filings`文件夹`sec_preprocessing`笔记本中的数据源详细信息和下载说明及预处理代码示例。）

## 预处理-句子检测和 n-grams

每个归档都是一个单独的文本文件，主索引包含归档元数据。我们摘录了信息量最大的章节，即：

*   项目 1 和 1A：业务和风险因子
*   项目 7：管理层的讨论
*   项目 7a：关于市场风险的披露

`sec_preprocessing`笔记本展示了如何使用 spaCy 解析和标记文本，类似于*第 14 章中的方法。*我们不会对标记进行柠檬化，以保留单词用法的细微差别。

### 自动短语检测

与上一节一样，我们使用 Gensim 检测由多个标记或 n-gram 组成的短语。笔记本显示，最常见的 Bigram 包括`common_stock`、`united_states`、`cash_flows`、`real_estate`和`interest_rates`。

我们最终得到的词汇表略多于 201000 个，平均频率为 7，这表明我们可以在训练 word2vec 模型时通过增加最小频率来消除大量噪音。

### 用回报率标记文件以预测意外收益

该数据集附带了与 10000 份文件相关的报价单和提交日期列表。我们可以使用此信息选择围绕申报公告的特定时期的股票价格。目标是训练一个模型，该模型使用给定归档的单词向量作为输入，预测归档后的返回。

下面的代码示例显示了如何在提交后的期间内使用 1 个月的报税表标记单个提交：

```py
with pd.HDFStore(DATA_FOLDER / 'assets.h5') as store:
    prices = store['quandl/wiki/prices'].adj_close
sec = pd.read_csv('sec_path/filing_index.csv').rename(columns=str.lower)
sec.date_filed = pd.to_datetime(sec.date_filed)
sec = sec.loc[sec.ticker.isin(prices.columns), ['ticker', 'date_filed']]
price_data = []
for ticker, date in sec.values.tolist():
    target = date + relativedelta(months=1)
    s = prices.loc[date: target, ticker]
    price_data.append(s.iloc[-1] / s.iloc[0] - 1)
df = pd.DataFrame(price_data,
                  columns=['returns'],
                  index=sec.index) 
```

在接下来的章节中，当我们研究深度学习架构时，我们将回到这个。

## 模型训练

`gensim.models.word2vec`类实现了前面介绍的 skip gram 和 CBOW 体系结构。笔记本`word2vec`包含其他实现细节。

为了便于记忆高效的文本摄取，`LineSentence`类根据文本文件中包含的各个句子创建生成器：

```py
sentence_path = Path('data', 'ngrams', f'ngrams_2.txt')
sentences = LineSentence(sentence_path) 
```

`Word2Vec`类提供了本章前面介绍的配置选项：

```py
model = Word2Vec(sentences,
                 sg=1,          # 1=skip-gram; otherwise CBOW
                 hs=0,          # hier. softmax if 1, neg. sampling if 0
                 size=300,      # Vector dimensionality
                 window=3,      # Max dist. btw target and context word
                 min_count=50,  # Ignore words with lower frequency
                 negative=10,   # noise word count for negative sampling
                 workers=8,     # no threads
                 iter=1,        # no epochs = iterations over corpus
                 alpha=0.025,   # initial learning rate
                 min_alpha=0.0001 # final learning rate
                ) 
```

笔记本显示了如何保存和重新加载模型以继续训练，或者如何单独存储嵌入向量，例如，用于机器学习模型。

### 模型评估

基本功能包括识别相似单词：

```py
sims=model.wv.most_similar(positive=['iphone'], restrict_vocab=15000)
                  term   similarity
0                 ipad    0.795460
1              android    0.694014
2           smartphone    0.665732 
```

我们还可以相应地使用正贡献和负贡献来验证个人类比：

```py
model.wv.most_similar(positive=['france', 'london'],
                      negative=['paris'],
                      restrict_vocab=15000)
             term  similarity
0  united_kingdom    0.606630
1         germany    0.585644
2     netherlands    0.578868 
```

### 参数设置对性能的影响

我们可以使用类比来评估不同参数设置的影响。以下结果突出（详见`models`文件夹中的详细结果）：

*   负采样优于分层 softmax，同时训练速度也更快。
*   skip-gram 体系结构优于 CBOW。
*   不同的`min_count`设置影响较小；50 的中点表现最好。

使用负采样和`min_count`为 50 的最佳性能 skip gram 模型进行的进一步实验表明：

*   小于 5 的上下文窗口会降低性能。
*   较高的负采样率会提高性能，但会降低培训速度。
*   更大的向量可以提高性能，其`size`为 600，最高精确度为 38.5%。

# 使用 doc2vec 嵌入的情感分析

文本分类需要组合多个单词嵌入。一种常见的方法是平均文档中每个单词的嵌入向量。这将使用来自所有嵌入的信息，并有效地使用向量加法来到达嵌入空间中的不同位置点。但是，有关单词顺序的相关信息丢失。

相反，word2vec 作者在发表其原始贡献后不久开发的文档嵌入模型 doc2vec 直接为段落或产品评论等文本片段生成嵌入。与 word2vec 类似，doc2vec 也有两种风格：

*   **分布式字袋**（**DBOW**模型对应于 word2vec CBOW 模型。文档向量是在基于上下文词向量和文档文档文档向量预测目标词的合成任务上对网络进行训练的结果。
*   **分布式内存**（**DM**模型对应 word2wec skip gram 架构。文档向量是通过训练神经网络，使用完整文档的文档向量预测目标词而得到的。

Gensim 的`Doc2Vec`类实现了该算法。我们将通过将 doc2vec 应用于我们在*第 14 章*中介绍的 Yelp 情感数据集来说明它的使用。为了加快培训，我们将数据限制在 50 万 Yelp 评论及其相关星级的分层随机样本中。`doc2vec_yelp_sentiment`笔记本包含本节的代码示例。

## 从 Yelp 情感数据创建 doc2vec 输入

我们加载包含 600 万条评论的组合 Yelp 数据集，如*第 14 章*中创建的、*交易文本数据–情感分析*中所述，并为每个明星评级抽取 100000 条评论：

```py
df = pd.read_parquet('data_path / 'user_reviews.parquet').loc[:, ['stars', 
                                                                  'text']]
stars = range(1, 6)
sample = pd.concat([df[df.stars==s].sample(n=100000) for s in stars]) 
```

我们使用 nltk 的`RegexpTokenizer`进行简单快速的文本清理：

```py
tokenizer = RegexpTokenizer(r'\w+')
stopword_set = set(stopwords.words('english'))
def clean(review):
    tokens = tokenizer.tokenize(review)
    return ' '.join([t for t in tokens if t not in stopword_set])
sample.text = sample.text.str.lower().apply(clean) 
```

在我们筛选出少于 10 个标记的评论之后，剩下 485825 个样本。*图 16.6*的左面板显示了每次审核的令牌数量分布。

`gensim.models.Doc2Vec`类以`TaggedDocument`格式处理文档，该格式包含标记化文档以及唯一的标记，该标记允许在培训后访问文档向量：

```py
sample = pd.read_parquet('yelp_sample.parquet')
sentences = []
for i, (stars, text) in df.iterrows():
    sentences.append(TaggedDocument(words=text.split(), tags=[i])) 
```

## 培训 doc2vec 模型

培训界面的工作方式与 word2vec 类似，还允许继续培训和坚持：

```py
model = Doc2Vec(documents=sentences,
                dm=1,           # 1=distributed memory, 0=dist.BOW
                epochs=5,
                size=300,       # vector size
                window=5,       # max. distance betw. target and context
                min_count=50,   # ignore tokens w. lower frequency
                negative=5,     # negative training samples
                dm_concat=0,    # 1=concatenate vectors, 0=sum
                dbow_words=0,   # 1=train word vectors as well
                workers=4)
model.save((results_path / 'sample.model').as_posix()) 
```

我们可以查询与给定标记最相似的*n*术语，作为评估结果词向量的快速方法，如下所示：

```py
model.most_similar('good') 
```

*图 16.7*右侧面板显示返回的令牌及其相似性：

![](img/B15439_16_07.png)

图 16.7：每次评审的令牌数量直方图（左）和与令牌“良好”最相似的术语

## 用文档向量训练分类器

现在，我们可以访问文档向量，为情感分类器创建特征：

```py
y = sample.stars.sub(1)
X = np.zeros(shape=(len(y), size)) # size=300
for i in range(len(sample)):
    X[i] = model.docvecs[i]
X.shape
(485825, 300) 
```

我们像往常一样创建培训和测试集：

```py
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y) 
```

现在，我们继续训练一个`RandomForestClassifier`、一个 LightGBM 梯度提升模型和一个多项式逻辑回归。我们使用 500 棵树作为随机森林：

```py
rf = RandomForestClassifier(n_jobs=-1, n_estimators=500)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test) 
```

我们使用 LightGBM 分类器提前停止，但它运行了整整 5000 轮，因为它继续以提高其验证性能：

```py
train_data = lgb.Dataset(data=X_train, label=y_train)
test_data = train_data.create_valid(X_test, label=y_test)
params = {'objective': 'multiclass',
          'num_classes': 5}
lgb_model = lgb.train(params=params,
                      train_set=train_data,
                      num_boost_round=5000,
                      valid_sets=[train_data, test_data],
                      early_stopping_rounds=25,
                      verbose_eval=50)
# generate multiclass predictions
lgb_pred = np.argmax(lgb_model.predict(X_test), axis=1) 
```

最后，我们建立了一个多项式逻辑回归模型，如下所示：

```py
lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',
                        class_weight='balanced')
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test) 
```

当我们在验证集上计算每个模型的准确度时，梯度增强的性能显著提高，达到 62.24%。*图 16.8*显示了每种型号的混淆矩阵和精确度：

![](img/B15439_16_08.png)

图 16.8：备选模型的混淆矩阵和测试精度

*第 14 章**交易文本数据–情感分析*中的情感分类结果为 LightGBM 提供了更好的准确度（73.6%），但我们使用了完整的数据集并包含了额外的特征。您可能需要测试增加样本大小或调整模型参数是否会使 doc2vec 同样出色。

## 经验教训和下一步

本例使用 doc2vec 对**产品评论而非金融文档**进行情感分析。我们之所以选择产品评论，是因为很难找到足够大的金融文本数据，以便从头开始培训单词嵌入，并且还具有有用的情感标签或足够的信息，以便我们自己为它们指定标签，例如资产回报。

虽然产品审查允许我们演示工作流程，但我们需要记住**重要的结构差异**：产品审查通常是简短、非正式的，并且特定于一个特定对象。相比之下，许多金融文件更长、更正式，而且目标对象可能无法明确识别。金融新闻文章可能涉及多个目标，虽然企业披露可能有明确的来源，但也可能讨论竞争对手。例如，一份分析报告也可能讨论同一对象或主题的积极和消极方面。

简言之，对金融文件中表达的情感的解释通常需要一种更复杂、更细致、更精细的方法，从不同方面建立对内容含义的理解。决策者也常常关心了解模型是如何得出结论的。

这些挑战尚未解决，仍然是一个非常活跃的研究领域，尤其是由于缺乏合适的数据源而变得复杂。然而，自 2018 年以来显著提升 NLP 各项任务表现的最新突破表明，金融情感分析在未来几年也可能变得更加稳健。接下来我们将讨论这些创新。

# 新前沿-预培训变压器模型

Word2vec 和 GloVe 嵌入比 bag of words 方法捕获更多的语义信息。然而，它们只允许每个令牌的单一固定长度表示，而不区分特定于上下文的用法。为了解决诸如同一单词的多个意义（称为**多义**等）等未解决的问题，出现了几种新的模式，它们建立在**注意机制**之上，旨在学习更多语境化的单词嵌入（Vaswani et al.，2017）。这些车型的主要特点如下：

*   使用**双向语言模型**处理文本（从左到右和从右到左），以获得更丰富的上下文表示
*   在大型通用语料库上使用**半监督预训练**以嵌入和网络权重的形式学习通用语言方面，这些嵌入和网络权重可以用于特定任务并进行微调（一种**迁移学习**形式，我们将在*第 18 章*中详细讨论，*用于金融时间序列和卫星图像的 CNN*

在本节中，我们简要描述注意机制，概述最近的 transformer模型如何从 Transformers（**BERT**）的**双向编码器表示开始—使用它来提高关键 NLP 任务的性能，参考一些预训练语言模型的来源，并解释如何将其用于金融情感分析。**

## 注意力是你所需要的

**注意机制**明确模拟句子中单词之间的关系，以便更好地融入上下文。它最初应用于机器翻译（Bahdanau、Cho 和 Bengio，2016），但此后已成为各种任务的神经语言模型的组成部分。

到 2017 年为止，**递归神经网络**（**RNNs**）以从左到右或从右到左的顺序处理文本，代表了翻译等 NLP 任务的最新水平。例如，谷歌自 2016 年末开始在生产中采用这种模式。顺序处理意味着在语义上连接远处单词的几个步骤，并排除了并行处理，这大大加快了在现代专用硬件（如 GPU）上的计算速度。（有关 RNN 的更多信息，请参见*第 19 章*、*RNN 的多变量时间序列和情感分析*。）

相比之下，在开创性论文*中引入的**Transformer**模型*（Vaswani 等人，2017 年）只需要确定语义相关单词的固定步骤。它依赖于一种自我注意机制，捕捉句子中所有单词之间的联系，而不管它们的相对位置如何。该模型通过为句子中的每个其他单词分配注意分数来学习单词的表示，该分数确定每个其他单词对表示的贡献程度。然后，这些分数通知所有单词表示的加权平均值，该值被输入到一个完全连接的网络中，以生成目标单词的新表示。

Transformer 模型使用具有多个层的编码器-解码器架构，每个层并行使用多个注意机制（称为**头**。它在各种翻译任务上取得了巨大的性能改进，更重要的是，它激发了一股新的研究浪潮，对处理更广泛任务的神经语言模型进行了研究。GitHub 上链接的资源包含各种关于注意机制如何工作的优秀视觉解释，因此我们在此不再详细介绍。

## BERT–迈向更通用的语言模型

2018 年，谷歌发布了**伯特**模型，代表变压器的**双向编码器表示（Devlin 等人，2019）。作为 NLP 研究的重大突破，它在 11 项自然语言理解任务上取得了突破性成果，从问答和命名实体识别到释义和情感分析，正如**通用语言理解评估**（**胶**所测量的基准测试（请参阅 GitHub 以获取任务描述和排行榜的链接）。**

伯特提出的新想法引发了一系列新的研究，产生了数十项改进，很快在胶水任务上超过了非专家人类，并导致了由 DeepMind 设计的更具挑战性的**超级胶水**基准测试（Wang et al.，2019）。因此，2018 年被认为是 NLP 研究的转折点；谷歌搜索和微软的必应现在都在使用 BERT 的变体来解释用户查询并提供更准确的结果。

我们将简要介绍 BERT 的主要创新，并说明如何开始使用它，以及它的后续增强功能，其中一个开源库提供预训练模型。

### 关键创新——更深入的关注和预培训

BERT 模型建立在**两个关键思想**的基础上，即上一节中描述的**变压器架构**和**无监督预培训**，因此不需要为每个新任务从头开始进行培训；相反，它的重量经过了微调：

*   BERT 将**注意机制**提升到一个新的（更深的）层次，根据结构，使用 12 或 24 层，每个层有 12 或 16 个注意头。这将导致多达 24×16=384 个注意机制来学习特定于上下文的嵌入。
*   BERT 使用**无监督、双向预训练**在两项任务中提前学习其权重：**掩蔽语言建模**（根据左右上下文预测缺失单词）和**下一句预测**（预测一句是否紧跟另一句）。

**上下文无关**模型如 word2vec 或 GloVe 为词汇表中的每个单词生成一个嵌入：单词“bank”在“bank account”和“bank of the river”中具有相同的上下文无关表示。相反，BERT 学习基于句子中的其他单词来表示每个单词。作为一个**双向模型**，BERT 能够在句子“我访问了银行账户”中表示“银行”一词，不仅基于“我访问了”作为单向上下文模型，而且基于“账户”

伯特及其继任者可以在维基百科这样的通用语料库上进行**预训练，然后根据特定任务调整其最后一层，并**微调其权重**。因此，您可以使用具有数十亿参数的大规模、最先进的模型，而只需花费几个小时，而不是几天或几周的培训成本。有几个库提供了这样的预训练模型，您可以在此基础上为选择的数据集开发自定义情感分类器。**

### 使用经过预训练的最先进模型

本节描述的最近 NLP 突破展示了如何从未标记文本中获取语言知识，网络足够大，能够代表罕见使用现象的长尾。由此产生的转换器架构对词序和上下文的假设较少；相反，他们使用数亿甚至数十亿个参数，从大量数据中学习对语言更微妙的理解。

我们将重点介绍几个提供预训练网络的库，以及优秀的 Python 教程。

#### 拥抱面变形金刚图书馆

Hugging Face 是一家美国初创公司，开发聊天机器人应用程序，旨在提供个性化的人工智能通信。2019 年末，它筹集了 1500 万美元，以进一步开发其非常成功的开源 NLP 库 Transformers。

该库提供了用于自然语言理解和生成的通用体系结构，具有超过 100 种语言的 32 个预训练模型，以及 TensorFlow 2 和 PyTorch 之间的深度互操作性。它有优秀的文档。

spacy transformers 库包括包装器，以便于在 spacy 管道中包含预训练的变压器模型。有关更多信息，请参阅 GitHub 上的参考链接。

#### 阿连利普

AlnEnLP 是由艾伦研究所（AcInstitute）由 AI 公司的微软公司水果挞 Paul Allen（MealT1）发起的，由华盛顿大学的研究人员密切合作。它被设计成一个研究图书馆，用于开发基于 PyTorch 的各种语言任务的最先进的深度学习模型。

它为从问答到句子注释的关键任务提供了解决方案，包括阅读理解、命名实体识别和情感分析。预训练的**RoBERTa**模型（更健壮的版本的 BERT；Liu 等人，2019 年）在斯坦福情感树库上实现了 95%以上的准确率，只需几行代码即可使用（参见 GitHub 上的文档链接）。

## 文本数据交易–经验教训和下一步

正如*节末尾所强调的，使用 doc2vec 嵌入*进行情感分析时，金融文档的一些重要结构特征通常会使其解释复杂化，并破坏简单的基于词典的方法。

在最近的一项金融情感分析调查中，Man、Luo 和 Lin（2019）发现，大多数现有方法仅识别高水平的极性，如积极、消极或中性。然而，导致真正决策的实际应用通常需要更细致和透明的分析。此外，缺乏具有相关标签的大型金融文本数据集限制了使用传统机器学习方法或神经网络进行情感分析的可能性。

刚才描述的预训练方法，原则上，可以加深对文本信息的理解，因此提供了实质性的希望。然而，大多数使用 transformers 的应用研究都集中在 NLP 任务上，如翻译、问答、逻辑或对话系统。与金融数据相关的应用程序仍处于初级阶段（例如，见 Araci 2019）。考虑到预训练模型的可用性及其从金融文本数据中提取更有价值信息的潜力，这种情况可能很快就会改变。

# 总结

在本章中，我们讨论了一种生成文本特征的新方法，该方法使用浅层神经网络进行无监督机器学习。我们看到了由此产生的单词嵌入是如何通过捕获一些使用它们的上下文来捕获单个标记意义之外的有趣语义方面的。我们还介绍了如何使用类比和线性代数来评估单词向量的质量。

我们使用 Keras 构建产生这些功能的网络架构，并将更高性能的 Gensim 实现应用于金融新闻和 SEC 文件。尽管数据集相对较小，word2vec 嵌入确实捕获了有意义的关系。我们还演示了如何使用股票价格数据进行适当的标记，以形成监督学习的基础。

我们应用 doc2vec 算法（生成文档而不是令牌向量）来构建基于 Yelp 业务评论的情感分类器。虽然这不太可能产生可交易的信号，但它说明了如何从相关文本数据中提取特征并训练模型以预测可能对交易策略有用的结果的过程。

最后，我们概述了最近的研究突破，这些突破有望产生更强大的自然语言模型，因为可以使用只需要微调的预训练体系结构。然而，金融数据的应用仍处于研究前沿。

在下一章中，我们将深入到本书的最后一部分，其中涵盖了各种深度学习架构如何对算法交易有用。